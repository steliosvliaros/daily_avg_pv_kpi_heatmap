{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0b44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# The notebooks are in a 'notebooks' subfolder, so workspace root is one level up\n",
    "# This works if notebook is in notebooks/ folder\n",
    "workspace_root = Path.cwd().parent.resolve()\n",
    "\n",
    "# Add workspace root to path so we can import from src\n",
    "sys.path.insert(0, str(workspace_root))\n",
    "\n",
    "WORKSPACE_ROOT = workspace_root\n",
    "DATA_DIR = WORKSPACE_ROOT / \"data\"\n",
    "DATA_XLSX = DATA_DIR / \"daily_energy.xlsx\"\n",
    "DATA_XLSX_HISTORICAL = DATA_DIR / \"daily_energy_historical.xlsx\"\n",
    "CACHE_DIR = WORKSPACE_ROOT / \"pvgis_cache\"\n",
    "PLOTS_DIR = WORKSPACE_ROOT / \"plots\"\n",
    "DATA_XLSX_HISTORICAL = DATA_DIR / \"daily_energy_historical.xlsx\"\n",
    "DATA_XLSX_TOTAL_IRR = DATA_DIR / \"Exported Data-20150101T000000.xlsx\"\n",
    "DATA_XLSX_TOTAL_FULL = DATA_DIR / \"column_full_export.xlsx\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src import pvgis_pi_heatmap\n",
    "importlib.reload(pvgis_pi_heatmap)  # Reload to pick up any changes\n",
    "\n",
    "from src.pvgis_pi_heatmap import make_random_greece_meta, compute_pi_anomaly, short_label, parse_kwp_from_header\n",
    "from src.utils import save_figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute sanitized column names with updated module preserving units\n",
    "import importlib\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "workspace_root = WORKSPACE_ROOT if 'WORKSPACE_ROOT' in globals() else Path(__file__).resolve().parents[2]\n",
    "src_path = workspace_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "import scada_column_sanitizer as scs\n",
    "importlib.reload(scs)\n",
    "\n",
    "columns = scs.read_columns_only(str(DATA_XLSX_TOTAL_FULL), sheet_name=0)\n",
    "cfg = scs.SanitizeConfig(prompt_missing_capacity=True, default_capacity_kwp=None)\n",
    "sanitizer = scs.ScadaColumnSanitizer(config=cfg)\n",
    "\n",
    "# Load existing mapping from current version (versioned mappings folder)\n",
    "mappings_dir = workspace_root / 'mappings'\n",
    "mappings_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Read current mapping pointer\n",
    "current_txt = mappings_dir / 'current.txt'\n",
    "if current_txt.exists():\n",
    "    current_mapping_file = current_txt.read_text(encoding='utf-8').strip()\n",
    "    current_mapping_path = mappings_dir / current_mapping_file\n",
    "    existing_mapping = sanitizer.load_mapping_csv(current_mapping_path)\n",
    "    print(f\"Loaded existing mapping: {current_mapping_file}\")\n",
    "else:\n",
    "    existing_mapping = {}\n",
    "    print(\"No existing mapping found (first run)\")\n",
    "\n",
    "# Regenerate with existing mappings to preserve unchanged columns\n",
    "sanitized, mapping = sanitizer.sanitize_columns(columns, existing_mapping=existing_mapping)\n",
    "\n",
    "print(\"\\nRecomputed sanitized columns. Showing first 10:\")\n",
    "for c in sanitized[:10]:\n",
    "    print(\" -\", c)\n",
    "\n",
    "# Determine next version number\n",
    "def get_next_version() -> int:\n",
    "    \"\"\"Find the next version number from existing versioned mappings.\"\"\"\n",
    "    version_pattern = re.compile(r'park_power_mapping_v(\\d+)\\.csv')\n",
    "    versions = []\n",
    "    for f in mappings_dir.glob('park_power_mapping_v*.csv'):\n",
    "        m = version_pattern.match(f.name)\n",
    "        if m:\n",
    "            versions.append(int(m.group(1)))\n",
    "    return max(versions, default=0) + 1\n",
    "\n",
    "next_version = get_next_version()\n",
    "new_mapping_filename = f'park_power_mapping_v{next_version:03d}.csv'\n",
    "new_mapping_path = mappings_dir / new_mapping_filename\n",
    "\n",
    "# Save new versioned mapping\n",
    "import csv\n",
    "new_mapping_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(new_mapping_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['original', 'sanitized'])\n",
    "    for orig, san in mapping.items():\n",
    "        writer.writerow([orig, san])\n",
    "\n",
    "print(f\"\\nâœ… Saved new mapping version: {new_mapping_filename}\")\n",
    "\n",
    "# Update current.txt pointer to new version\n",
    "current_txt.write_text(new_mapping_filename, encoding='utf-8')\n",
    "print(f\"âœ… Updated mappings/current.txt â†’ {new_mapping_filename}\")\n",
    "\n",
    "print(f\"\\nðŸ“Œ Mapping versioning system:\")\n",
    "print(f\"   Location: mappings/{new_mapping_filename}\")\n",
    "print(f\"   Active: Yes (current.txt points to this version)\")\n",
    "print(f\"   To rollback: edit mappings/current.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a3266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings root: C:\\00_Dev\\daily_avg_pv_kpi_heatmap\\mappings\n",
      "Current mapping: park_power_mapping_v004.csv\n",
      "No files found in inbox.\n"
     ]
    }
   ],
   "source": [
    "# Run bronze ingestion with versioned mapping support\n",
    "# Reads mappings/current.txt to determine active mapping file\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "from src import bronze_ingest as bi\n",
    "\n",
    "# Reload module to pick up any code changes\n",
    "importlib.reload(bi)\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = WORKSPACE_ROOT / \"data\"\n",
    "BRONZE_ROOT = WORKSPACE_ROOT / \"bronze\"\n",
    "MAPPINGS_ROOT = WORKSPACE_ROOT / \"mappings\"\n",
    "\n",
    "# Ensure expected folders exist\n",
    "for p in [\n",
    "    DATA_ROOT / \"inbox\",\n",
    "    DATA_ROOT / \"processing\",\n",
    "    DATA_ROOT / \"archived\",\n",
    "    DATA_ROOT / \"rejected\",\n",
    "    DATA_ROOT / \"_locks\",\n",
    "    BRONZE_ROOT,\n",
    "    MAPPINGS_ROOT,\n",
    "]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build config (min_age_seconds=0 to ingest immediately in notebook)\n",
    "cfg = bi.Config(\n",
    "    data_root=DATA_ROOT,\n",
    "    inbox=DATA_ROOT / \"inbox\",\n",
    "    processing=DATA_ROOT / \"processing\",\n",
    "    archived=DATA_ROOT / \"archived\",\n",
    "    rejected=DATA_ROOT / \"rejected\",\n",
    "    bronze_root=BRONZE_ROOT,\n",
    "    mappings_root=MAPPINGS_ROOT,\n",
    "    dataset_name=\"scada_1d_signal\",\n",
    "    timezone_local=\"Europe/Athens\",\n",
    "    daily_interval_end_is_midnight=True,\n",
    "    parquet_compression=\"zstd\",\n",
    "    min_age_seconds=0,\n",
    "    stable_check_seconds=0,\n",
    "    sheet_name=None,\n",
    "    csv_sep=None,\n",
    "    csv_encoding=None,\n",
    ")\n",
    "cfg.lockfile = DATA_ROOT / \"_locks\" / \"bronze_ingest.lock\"\n",
    "\n",
    "# Run ingestion (reads mappings/current.txt to find active mapping, applies it, writes bronze with mapping metadata)\n",
    "print(f\"Mappings root: {MAPPINGS_ROOT}\")\n",
    "print(f\"Current mapping: {(MAPPINGS_ROOT / 'current.txt').read_text().strip()}\")\n",
    "bi.ingest_folder(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2294e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New runs: ['20260122T134644Z']\n",
      "Parquet parts: 134\n",
      "Shape: (81132, 15)\n",
      "Columns: ['ts_local', 'ts_utc', 'interval_start_date', 'year', 'month', 'park_id', 'park_capacity_kwp', 'signal_name', 'unit', 'value', 'source_file', 'source_file_hash', 'run_id', 'ingested_at_utc', 'ingest_key']\n",
      "\n",
      "âœ… Sample validation (first 5 unique park_id + signal_name + capacity combinations):\n",
      "       park_id  park_capacity_kwp        signal_name  unit\n",
      "p_4e_ener_liko                176 pcc_acti_ener_expo u_kwh\n",
      "p_4e_ener_lexa               4472 pcc_acti_ener_expo u_kwh\n",
      "     frag_util               4866 pcc_acti_ener_expo u_kwh\n",
      "     hlia_andr                474 pcc_acti_ener_expo u_kwh\n",
      "     nyco_gian                993 pcc_acti_ener_expo u_kwh\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "bronze_root = WORKSPACE_ROOT / \"bronze\"\n",
    "dataset_name = \"scada_1d_signal\"\n",
    "registry = bronze_root / \"_ops\" / \"ingest_registry_files.csv\"\n",
    "\n",
    "df_reg = pd.read_csv(registry, dtype=str)\n",
    "df_reg = df_reg[(df_reg[\"dataset\"] == dataset_name) & (df_reg[\"status\"] == \"ingested\")]\n",
    "\n",
    "# ---- you maintain this (example) ----\n",
    "last_run_id = \"20260120T000000Z\"   # load from a small state file in real code\n",
    "\n",
    "new_runs = sorted(df_reg[df_reg[\"run_id\"] > last_run_id][\"run_id\"].unique())\n",
    "print(\"New runs:\", new_runs)\n",
    "\n",
    "# Find parquet files written by those runs (pattern includes run_id in filename)\n",
    "parts = []\n",
    "base = bronze_root / dataset_name\n",
    "for r in new_runs:\n",
    "    pattern = str(base / \"year=*\" / \"month=*\" / f\"part-run={r}-hash=*.parquet\")\n",
    "    parts.extend(glob.glob(pattern))\n",
    "\n",
    "print(\"Parquet parts:\", len(parts))\n",
    "\n",
    "if parts:\n",
    "    # Read each parquet file individually and concatenate to handle schema mismatches\n",
    "    dfs = [pd.read_parquet(p) for p in parts]\n",
    "    df_new = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Shape: {df_new.shape}\")\n",
    "    print(f\"Columns: {list(df_new.columns)}\")\n",
    "    \n",
    "    # âœ… Validate: Check sample rows to verify capacity and signal_name are correctly separated\n",
    "    print(\"\\nâœ… Sample validation (first 5 unique park_id + signal_name + capacity combinations):\")\n",
    "    sample = df_new[['park_id', 'park_capacity_kwp', 'signal_name', 'unit']].drop_duplicates().head()\n",
    "    print(sample.to_string(index=False))\n",
    "else:\n",
    "    df_new = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee8e118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_id</th>\n",
       "      <th>signal_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts_local</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00+02:00</th>\n",
       "      <td>p_4e_ener_liko</td>\n",
       "      <td>pcc_acti_ener_expo</td>\n",
       "      <td>44.878417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 00:00:00+02:00</th>\n",
       "      <td>p_4e_ener_liko</td>\n",
       "      <td>pcc_acti_ener_expo</td>\n",
       "      <td>358.194506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 00:00:00+02:00</th>\n",
       "      <td>p_4e_ener_liko</td>\n",
       "      <td>pcc_acti_ener_expo</td>\n",
       "      <td>628.599860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04 00:00:00+02:00</th>\n",
       "      <td>p_4e_ener_liko</td>\n",
       "      <td>pcc_acti_ener_expo</td>\n",
       "      <td>501.527094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05 00:00:00+02:00</th>\n",
       "      <td>p_4e_ener_liko</td>\n",
       "      <td>pcc_acti_ener_expo</td>\n",
       "      <td>36.300399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  park_id         signal_name       value\n",
       "ts_local                                                                 \n",
       "2015-01-01 00:00:00+02:00  p_4e_ener_liko  pcc_acti_ener_expo   44.878417\n",
       "2015-01-02 00:00:00+02:00  p_4e_ener_liko  pcc_acti_ener_expo  358.194506\n",
       "2015-01-03 00:00:00+02:00  p_4e_ener_liko  pcc_acti_ener_expo  628.599860\n",
       "2015-01-04 00:00:00+02:00  p_4e_ener_liko  pcc_acti_ener_expo  501.527094\n",
       "2015-01-05 00:00:00+02:00  p_4e_ener_liko  pcc_acti_ener_expo   36.300399"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## select one park and a signal to check time series\n",
    "park_id = 'p_4e_ener_liko'\n",
    "signal_name = \"pcc_acti_ener_expo\"\n",
    "## filter sample dataframe\n",
    "_df_new = df_new[['ts_local', 'park_id', 'signal_name', 'value']].set_index('ts_local')\n",
    "## filter _df_new for selected park and signal\n",
    "_df_new = _df_new[(_df_new['park_id'] == park_id) & (_df_new['signal_name'] == signal_name)]\n",
    "_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fd65e",
   "metadata": {},
   "source": [
    "## End of Bronze Database Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85366ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_daily_data(\n",
    "    excel_path: Path | str,\n",
    "    timestamp_col: str = \"Timestamp\",\n",
    "    drop_cols: list[str] | None = None,\n",
    "    sanitize: bool = True,\n",
    "    export_power_mapping: bool = True,\n",
    ") -> tuple[pd.DataFrame, list[str], pd.DataFrame | None]:\n",
    "    \"\"\"\n",
    "    Load daily energy data from Excel and prepare for analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    excel_path : Path | str\n",
    "        Path to Excel file with Timestamp and park columns\n",
    "    timestamp_col : str\n",
    "        Name of timestamp column (default: \"Timestamp\")\n",
    "    drop_cols : list[str] | None\n",
    "        Columns to exclude from park data (default: [\"Timestamp\", \"date\"])\n",
    "    sanitize : bool\n",
    "        If True, sanitize column names and standardize power ratings (default: True)\n",
    "    export_power_mapping : bool\n",
    "        If True and sanitize=True, export power mapping to CSV (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.DataFrame, list[str], pd.DataFrame | None]\n",
    "        (date-indexed DataFrame, list of park column names, power_mapping_df or None)\n",
    "    \"\"\"\n",
    "    # Load Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Auto-detect timestamp column if it has a different name\n",
    "    actual_timestamp_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ('timestamp', 'time', 'date', 'datetime'):\n",
    "            actual_timestamp_col = col\n",
    "            break\n",
    "    \n",
    "    if actual_timestamp_col is None:\n",
    "        actual_timestamp_col = timestamp_col\n",
    "    \n",
    "    # Normalize timestamp\n",
    "    df[actual_timestamp_col] = pd.to_datetime(df[actual_timestamp_col])\n",
    "    df[\"date\"] = df[actual_timestamp_col].dt.normalize()\n",
    "    \n",
    "    # Define columns to exclude\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [actual_timestamp_col, \"date\"]\n",
    "    \n",
    "    # Extract park columns (all except dropped)\n",
    "    park_cols = [c for c in df.columns if c not in drop_cols]\n",
    "    \n",
    "    # Sanitize column names and extract power ratings\n",
    "    power_mapping = None\n",
    "    if sanitize:\n",
    "        df_sanitized, power_mapping = sanitize_dataframe_columns(df[park_cols])\n",
    "        park_cols = list(df_sanitized.columns)\n",
    "        df[park_cols] = df_sanitized\n",
    "        \n",
    "        # Export power mapping to CSV\n",
    "        if export_power_mapping:\n",
    "            power_export_path = WORKSPACE_ROOT / \"outputs\" / \"park_power_mapping.csv\"\n",
    "            power_export_path.parent.mkdir(exist_ok=True)\n",
    "            power_mapping.to_csv(power_export_path)\n",
    "            print(f\"âœ… Power mapping exported to: {power_export_path}\")\n",
    "    \n",
    "    # Create date-indexed DataFrame\n",
    "    daily = df.set_index(\"date\")[park_cols].sort_index()\n",
    "    \n",
    "    return daily, park_cols, power_mapping\n",
    "\n",
    "\n",
    "# # Load current and historical daily data\n",
    "# print(\"ðŸ“‚ Loading daily energy data...\")\n",
    "# daily, park_cols, daily_power_map = load_and_prepare_daily_data(DATA_XLSX)\n",
    "# print(f\"   âœ“ Current: {len(daily)} rows Ã— {len(park_cols)} parks\")\n",
    "\n",
    "print(\"ðŸ“‚ Loading historical daily energy data...\")\n",
    "daily_historical, _park_cols, historical_power_map = load_and_prepare_daily_data(DATA_XLSX_TOTAL_IRR)\n",
    "print(f\"   âœ“ Historical: {len(daily_historical)} rows Ã— {len(_park_cols)} parks\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nðŸ“Š Historical data (head):\")\n",
    "print(daily_historical.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531061f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export park column names to CSV\n",
    "output_path = WORKSPACE_ROOT / \"outputs\" / \"park_columns.csv\"\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Create DataFrame with park names\n",
    "park_cols_df = pd.DataFrame({\n",
    "    'park_name': _park_cols\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "park_cols_df.to_csv(output_path, index=False)\n",
    "print(f\"âœ“ Exported {len(_park_cols)} park columns to: {output_path}\")\n",
    "print(f\"\\nFirst 10 parks:\")\n",
    "print(park_cols_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_park_name_before_pcc(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract park name and capacity, stopping before 'PCC'.\n",
    "    \n",
    "    Examples:\n",
    "    - '[Park_A_100_kWp_PCC_PCC_active_energy]' â†’ 'Park_A_100_kWp'\n",
    "    - '[Park_B_200_kWp_Location]' â†’ 'Park_B_200_kWp_Location'\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Remove brackets\n",
    "    clean = col.replace('[', '').replace(']', '').strip()\n",
    "    # Split at first PCC and take the part before it\n",
    "    parts = clean.split('_PCC')\n",
    "    if parts:\n",
    "        return parts[0]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_if_all_from_text_in_list(\n",
    "    columns: list[str],\n",
    "    text_list: list[str]\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Select columns that contain all specified texts from a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    columns : list[str]\n",
    "        List of column names to filter\n",
    "    text_list : list[str]\n",
    "        List of texts that must all be present in a column name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Filtered list of column names containing all specified texts\n",
    "    \"\"\"\n",
    "    selected_cols = []\n",
    "    for col in columns:\n",
    "        if all(text.lower() in col.lower() for text in text_list):\n",
    "            selected_cols.append(col)\n",
    "    return selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_historical = daily_historical[select_columns_if_all_from_text_in_list(daily_historical.columns, ['energy_exp'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f37aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot_timeseries_per_column(\n",
    "    df: pd.DataFrame,\n",
    "    title_prefix: str = \"Time Series\",\n",
    "    ylabel: str = \"Value\",\n",
    "    ncols: int = 3,\n",
    "    sharex: bool = True,\n",
    "    sharey: bool = False,\n",
    "    save: bool = False,\n",
    "    save_dir: str | Path | None = None,\n",
    "    base_filename: str | None = None,\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot one line chart per column in a grid of subplots and optionally save the figure.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        Date-indexed DataFrame; each column is a park/series.\n",
    "    title_prefix: str\n",
    "        Prefix used in subplot titles and default filename.\n",
    "    ylabel: str\n",
    "        Y-axis label for all subplots (default: \"Value\").\n",
    "    ncols: int\n",
    "        Number of columns in the subplot grid.\n",
    "    sharex/sharey: bool\n",
    "        Share axes across subplots.\n",
    "    save: bool\n",
    "        If True, saves the figure.\n",
    "    save_dir: str | Path | None\n",
    "        Directory where the figure will be saved. If None and `save=True`,\n",
    "        pass a directory explicitly (e.g., PLOTS_DIR) or it will fallback to CWD/\"plots\".\n",
    "    base_filename: str | None\n",
    "        Base filename without extension; if None, derived from title_prefix.\n",
    "    dpi: int\n",
    "        Resolution for the saved image.\n",
    "    fmt: str\n",
    "        File format for saving (e.g., \"png\", \"pdf\", \"svg\").\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    if len(cols) == 0:\n",
    "        print(\"No columns to plot.\")\n",
    "        return None\n",
    "\n",
    "    n = len(cols)\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    figsize = (min(6 * ncols, 24), max(2.8 * nrows, 4))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, sharex=sharex, sharey=sharey)\n",
    "    if nrows == 1:\n",
    "        axes = np.array([axes])\n",
    "    if ncols == 1:\n",
    "        axes = axes.reshape(nrows, 1)\n",
    "\n",
    "    axes_flat = axes.ravel()\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        ax = axes_flat[i]\n",
    "        ax.plot(df.index, df[col], linewidth=1.2)\n",
    "        try:\n",
    "            label = extract_park_name_before_pcc(col)\n",
    "        except Exception:\n",
    "            label = col\n",
    "        ax.set_title(f\"{title_prefix}: {label}\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    for j in range(len(cols), len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "\n",
    "    for ax in axes_flat[:len(cols)]:\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "            tick.set_ha('right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Use shared save utility\n",
    "    saved_path = save_figure(\n",
    "        fig=fig,\n",
    "        title_prefix=title_prefix,\n",
    "        save=save,\n",
    "        save_dir=save_dir,\n",
    "        base_filename=base_filename,\n",
    "        dpi=dpi,\n",
    "        fmt=fmt,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    return saved_path\n",
    "\n",
    "_ = lineplot_timeseries_per_column(\n",
    "    daily_historical,\n",
    "    title_prefix=\"Daily Energy\",\n",
    "    ylabel=\"Energy [kWh]\",\n",
    "    ncols=3,\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    save=True,\n",
    "    save_dir=PLOTS_DIR / \"weekly_analysis\",\n",
    "    base_filename=\"daily_energy_timeseries_grid\",\n",
    "    dpi=180,\n",
    "    fmt=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb35e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histplot_distribution_per_column(\n",
    "    df: pd.DataFrame,\n",
    "    title_prefix: str = \"Distribution\",\n",
    "    xlabel: str = \"Value\",\n",
    "    ncols: int = 3,\n",
    "    bins: int = 30,\n",
    "    density: bool = False,\n",
    "    dropna: bool = True,\n",
    "    sharex: bool = False,\n",
    "    sharey: bool = False,\n",
    "    show_stats: bool = True,\n",
    "    save: bool = False,\n",
    "    save_dir: str | Path | None = None,\n",
    "    base_filename: str | None = None,\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot one histogram per column in a grid of subplots and optionally save the figure.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame indexed by date; each column is a park/series.\n",
    "    title_prefix: str\n",
    "        Prefix used in subplot titles and default filename.\n",
    "    xlabel: str\n",
    "        X-axis label for all subplots (default: \"Value\").\n",
    "    ncols: int\n",
    "        Number of columns in the subplot grid.\n",
    "    bins: int\n",
    "        Histogram bin count.\n",
    "    density: bool\n",
    "        If True, normalize histogram to form a probability density.\n",
    "    dropna: bool\n",
    "        If True, exclude NaNs from each column.\n",
    "    sharex/sharey: bool\n",
    "        Share axes across subplots.\n",
    "    show_stats: bool\n",
    "        If True, draw vertical lines for mean and median.\n",
    "    save: bool\n",
    "        If True, saves the figure.\n",
    "    save_dir: str | Path | None\n",
    "        Directory where the figure will be saved. If None and `save=True`,\n",
    "        pass a directory explicitly (e.g., PLOTS_DIR) or it will fallback to CWD/\"plots\".\n",
    "    base_filename: str | None\n",
    "        Base filename without extension; if None, derived from title_prefix.\n",
    "    dpi: int\n",
    "        Resolution for the saved image.\n",
    "    fmt: str\n",
    "        File format for saving (e.g., \"png\", \"pdf\", \"svg\").\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    if len(cols) == 0:\n",
    "        print(\"No columns to plot.\")\n",
    "        return None\n",
    "\n",
    "    n = len(cols)\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    figsize = (min(6 * ncols, 24), max(2.8 * nrows, 4))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, sharex=sharex, sharey=sharey)\n",
    "    if nrows == 1:\n",
    "        axes = np.array([axes])\n",
    "    if ncols == 1:\n",
    "        axes = axes.reshape(nrows, 1)\n",
    "\n",
    "    axes_flat = axes.ravel()\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        ax = axes_flat[i]\n",
    "        values = df[col]\n",
    "        if dropna:\n",
    "            values = values.dropna()\n",
    "\n",
    "        ax.hist(values, bins=bins, alpha=0.75, color='steelblue', edgecolor='white', density=density)\n",
    "\n",
    "        try:\n",
    "            label = extract_park_name_before_pcc(col)\n",
    "        except Exception:\n",
    "            label = col\n",
    "        ax.set_title(f\"{title_prefix}: {label}\")\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(\"Density\" if density else \"Count\")\n",
    "\n",
    "        if show_stats and len(values) > 0:\n",
    "            mean_val = float(values.mean())\n",
    "            median_val = float(values.median())\n",
    "            ax.axvline(mean_val, color='orange', linestyle='--', linewidth=1, label='Mean')\n",
    "            ax.axvline(median_val, color='crimson', linestyle='--', linewidth=1, label='Median')\n",
    "            ax.legend(fontsize=8)\n",
    "\n",
    "    # Hide any unused axes\n",
    "    for j in range(len(cols), len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Use shared save utility\n",
    "    saved_path = save_figure(\n",
    "        fig=fig,\n",
    "        title_prefix=title_prefix,\n",
    "        save=save,\n",
    "        save_dir=save_dir,\n",
    "        base_filename=base_filename,\n",
    "        dpi=dpi,\n",
    "        fmt=fmt,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    return saved_path\n",
    "\n",
    "# Example: plot one histogram per column\n",
    "_ = histplot_distribution_per_column(\n",
    "    daily_historical,\n",
    "    title_prefix=\"Daily Energy Distribution\",\n",
    "    xlabel=\"Energy [kWh]\",\n",
    "    ncols=3,\n",
    "    bins=40,\n",
    "    density=False,\n",
    "    save=False,\n",
    "    save_dir=PLOTS_DIR,\n",
    "    base_filename=\"daily_energy_hist_grid\",\n",
    "    dpi=180,\n",
    "    fmt=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf51e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptive_statistics(\n",
    "    df: pd.DataFrame,\n",
    "    round_digits: int = 2,\n",
    "    cmap: str = \"Blues\",\n",
    "    sort_by: str | None = \"Mean\",\n",
    "    ascending: bool = False,\n",
    "    top_n: int | None = None,\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute and display descriptive statistics with a styled, visual summary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame indexed by date; columns are series/parks.\n",
    "    round_digits : int\n",
    "        Number of decimal places for numeric formatting.\n",
    "    cmap : str\n",
    "        Colormap name for background gradients.\n",
    "    sort_by : str | None\n",
    "        Optional column to sort by (e.g., \"Mean\", \"Median\", \"Missing %\").\n",
    "    ascending : bool\n",
    "        Sort ascending or descending when `sort_by` is provided.\n",
    "    top_n : int | None\n",
    "        If provided, display only the top N rows after sorting.\n",
    "    show : bool\n",
    "        If True, renders a styled table; otherwise returns the (stats, styler).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame | tuple[pd.DataFrame, pd.io.formats.style.Styler]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from IPython.display import display\n",
    "\n",
    "    # Base descriptive stats\n",
    "    desc = df.describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "\n",
    "    # Additional metrics\n",
    "    median = df.median()\n",
    "    var = df.var()\n",
    "    skew = df.skew()\n",
    "    kurt = df.kurtosis()\n",
    "\n",
    "    missing_count = df.isna().sum()\n",
    "    total_rows = df.shape[0]\n",
    "    missing_pct = (missing_count / total_rows * 100.0).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    non_nan_count = df.notna().sum()\n",
    "    zero_count = df.eq(0).sum()\n",
    "    zero_pct = (zero_count / non_nan_count * 100.0).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Assemble\n",
    "    stats = pd.DataFrame({\n",
    "        \"Count\": desc[\"count\"],\n",
    "        \"Missing Count\": missing_count,\n",
    "        \"Missing %\": missing_pct,\n",
    "        \"Zero Count\": zero_count,\n",
    "        \"Zero %\": zero_pct,\n",
    "        \"Mean\": desc[\"mean\"],\n",
    "        \"Median\": median,\n",
    "        \"Std\": desc[\"std\"],\n",
    "        \"Variance\": var,\n",
    "        \"Skew\": skew,\n",
    "        \"Kurtosis\": kurt,\n",
    "        \"Min\": desc[\"min\"],\n",
    "        \"Q1 (25%)\": desc[\"25%\"],\n",
    "        \"Q3 (75%)\": desc[\"75%\"],\n",
    "        \"Max\": desc[\"max\"],\n",
    "    })\n",
    "\n",
    "    # Optional sort and limit\n",
    "    if sort_by is not None and sort_by in stats.columns:\n",
    "        stats = stats.sort_values(by=sort_by, ascending=ascending)\n",
    "    if top_n is not None and top_n > 0:\n",
    "        stats = stats.head(top_n)\n",
    "\n",
    "    # Round for display\n",
    "    stats_rounded = stats.round(round_digits)\n",
    "\n",
    "    # Build a Styler with visual cues\n",
    "    subset_gradient = [\n",
    "        \"Mean\", \"Median\", \"Std\", \"Variance\", \"Skew\", \"Kurtosis\", \"Min\", \"Q1 (25%)\", \"Q3 (75%)\", \"Max\"\n",
    "    ]\n",
    "    subset_bar = [\"Missing %\", \"Zero %\"]\n",
    "\n",
    "    styler = (\n",
    "        stats_rounded.style\n",
    "        .set_caption(\"Descriptive Statistics per Series\")\n",
    "        .background_gradient(cmap=cmap, subset=[c for c in subset_gradient if c in stats_rounded.columns])\n",
    "        .bar(subset=[c for c in subset_bar if c in stats_rounded.columns], color=\"#FDAE61\")\n",
    "        .format(precision=round_digits)\n",
    "        .set_table_styles([\n",
    "            {\"selector\": \"th.col_heading\", \"props\": \"text-align: center;\"},\n",
    "            {\"selector\": \"th.row_heading\", \"props\": \"text-align: left;\"},\n",
    "            {\"selector\": \"caption\", \"props\": \"caption-side: top; font-weight: bold; font-size: 1.1em;\"},\n",
    "        ])\n",
    "        .set_properties(**{\"text-align\": \"right\"})\n",
    "    )\n",
    "\n",
    "    if show:\n",
    "        display(styler)\n",
    "        return stats\n",
    "    else:\n",
    "        return stats, styler\n",
    "\n",
    "# Render styled stats by default\n",
    "_ = compute_descriptive_statistics(daily_historical, round_digits=2, cmap=\"Blues\", sort_by=\"Missing %\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_nan_count(\n",
    "    df: pd.DataFrame,\n",
    "    title: str = \"NaN Count per Column\",\n",
    "    save: bool = False,\n",
    "    save_dir: str | Path | None = None,\n",
    "    base_filename: str | None = None,\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bar plot showing the count and percentage of NaN values for each column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe to analyze\n",
    "    title: str\n",
    "        Plot title\n",
    "    save: bool\n",
    "        If True, saves the figure\n",
    "    save_dir: str | Path | None\n",
    "        Directory where the figure will be saved. If None and `save=True`,\n",
    "        pass a directory explicitly (e.g., PLOTS_DIR) or it will fallback to CWD/\"plots\".\n",
    "    base_filename: str | None\n",
    "        Base filename without extension; if None, derived from title.\n",
    "    dpi: int\n",
    "        Resolution for the saved image.\n",
    "    fmt: str\n",
    "        File format for saving (e.g., \"png\", \"pdf\", \"svg\").\n",
    "    \"\"\"\n",
    "    nan_counts = df.isna().sum()\n",
    "    nan_pcts = (df.isna().sum() / len(df)) * 100\n",
    "    \n",
    "    # Sort by count descending\n",
    "    nan_counts = nan_counts.sort_values(ascending=False)\n",
    "    nan_pcts = nan_pcts[nan_counts.index]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar plot of counts\n",
    "    ax1.bar(range(len(nan_counts)), nan_counts.values, color='steelblue', alpha=0.7)\n",
    "    ax1.set_xticks(range(len(nan_counts)))\n",
    "    ax1.set_xticklabels([extract_park_name_before_pcc(col) for col in nan_counts.index], rotation=45, ha='right')\n",
    "    ax1.set_ylabel(\"NaN Count\")\n",
    "    ax1.set_xlabel(\"PV Parks\")\n",
    "    ax1.set_title(f\"{title} - Counts\")\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Bar plot of percentages\n",
    "    ax2.bar(range(len(nan_pcts)), nan_pcts.values, color='coral', alpha=0.7)\n",
    "    ax2.set_xticks(range(len(nan_pcts)))\n",
    "    ax2.set_xticklabels([extract_park_name_before_pcc(col) for col in nan_pcts.index], rotation=45, ha='right')\n",
    "    ax2.set_ylabel(\"NaN Percentage (%)\")\n",
    "    ax2.set_xlabel(\"PV Parks\")\n",
    "    ax2.set_title(f\"{title} - Percentages\")\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    ax2.axhline(y=50, color='red', linestyle='--', linewidth=1, label='50% threshold')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Use shared save utility\n",
    "    saved_path = save_figure(\n",
    "        fig=fig,\n",
    "        title_prefix=title,\n",
    "        save=save,\n",
    "        save_dir=save_dir,\n",
    "        base_filename=base_filename,\n",
    "        dpi=dpi,\n",
    "        fmt=fmt,\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return nan_counts, nan_pcts, saved_path\n",
    "\n",
    "nan_counts, nan_pcts, saved_path = barplot_nan_count(\n",
    "    daily_historical,\n",
    "    \"NaN Analysis for Daily Data\",\n",
    "    save=False,\n",
    "    save_dir=PLOTS_DIR,\n",
    "    base_filename=\"nan_analysis_daily\",\n",
    "    dpi=180,\n",
    "    fmt=\"png\",\n",
    ")\n",
    "print(f\"\\nTotal columns: {len(daily_historical.columns)}\")\n",
    "print(f\"Columns with NaN: {(nan_counts > 0).sum()}\")\n",
    "print(f\"Columns with >50% NaN: {(nan_pcts > 50).sum()}\")\n",
    "if saved_path:\n",
    "    print(f\"âœ“ Figure saved to: {saved_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf64906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_problematic_columns(df: pd.DataFrame, nan_threshold: float = 0.5, zero_threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    Detect columns that are mostly NaN, missing, or zeros.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe to analyze\n",
    "    nan_threshold: float\n",
    "        Fraction of NaN/missing values above which a column is flagged (default: 0.5 = 50%)\n",
    "    zero_threshold: float\n",
    "        Fraction of zero values (among non-NaN) above which a column is flagged (default: 0.8 = 80%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'mostly_nan': list of columns with high NaN percentage\n",
    "        - 'mostly_zero': list of columns with high zero percentage\n",
    "        - 'summary': DataFrame with statistics for each column\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'mostly_nan': [],\n",
    "        'mostly_zero': [],\n",
    "        'summary': []\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        total_count = len(df[col])\n",
    "        nan_count = df[col].isna().sum()\n",
    "        nan_pct = nan_count / total_count if total_count > 0 else 0\n",
    "        \n",
    "        # For zero detection, only consider non-NaN values\n",
    "        non_nan_values = df[col].dropna()\n",
    "        non_nan_count = len(non_nan_values)\n",
    "        \n",
    "        if non_nan_count > 0:\n",
    "            zero_count = (non_nan_values == 0).sum()\n",
    "            zero_pct = zero_count / non_nan_count\n",
    "        else:\n",
    "            zero_count = 0\n",
    "            zero_pct = 0\n",
    "        \n",
    "        # Store summary\n",
    "        results['summary'].append({\n",
    "            'column': col,\n",
    "            'total_rows': total_count,\n",
    "            'nan_count': nan_count,\n",
    "            'nan_pct': nan_pct,\n",
    "            'non_nan_count': non_nan_count,\n",
    "            'zero_count': zero_count,\n",
    "            'zero_pct': zero_pct,\n",
    "        })\n",
    "        \n",
    "        # Flag problematic columns\n",
    "        if nan_pct >= nan_threshold:\n",
    "            results['mostly_nan'].append(col)\n",
    "        \n",
    "        if zero_pct >= zero_threshold:\n",
    "            results['mostly_zero'].append(col)\n",
    "    \n",
    "    results['summary'] = pd.DataFrame(results['summary'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "problematic = detect_problematic_columns(daily_historical, nan_threshold=0.5, zero_threshold=0.8)\n",
    "\n",
    "print(\"=== Columns with mostly NaN (â‰¥50%) ===\")\n",
    "if problematic['mostly_nan']:\n",
    "    for col in problematic['mostly_nan']:\n",
    "        stats = problematic['summary'][problematic['summary']['column'] == col].iloc[0]\n",
    "        print(f\"  {col}: {stats['nan_pct']*100:.1f}% NaN ({stats['nan_count']}/{stats['total_rows']})\")\n",
    "else:\n",
    "    print(\"  None found\")\n",
    "\n",
    "print(\"\\n=== Columns with mostly zeros (â‰¥80% of non-NaN) ===\")\n",
    "if problematic['mostly_zero']:\n",
    "    for col in problematic['mostly_zero']:\n",
    "        stats = problematic['summary'][problematic['summary']['column'] == col].iloc[0]\n",
    "        print(f\"  {col}: {stats['zero_pct']*100:.1f}% zeros ({stats['zero_count']}/{stats['non_nan_count']} non-NaN)\")\n",
    "else:\n",
    "    print(\"  None found\")\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(problematic['summary'][['column', 'nan_pct', 'zero_pct']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(\n",
    "    mat: pd.DataFrame,\n",
    "    title: str,\n",
    "    ylabel: str = \"Production [kWh]\",\n",
    "    ylim=None,\n",
    "    save: bool = False,\n",
    "    save_dir: str | Path | None = None,\n",
    "    base_filename: str | None = None,\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create boxplots for each park showing the distribution of values across all dates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mat: pd.DataFrame\n",
    "        Date x park dataframe with values to plot\n",
    "    title: str\n",
    "        Plot title\n",
    "    ylabel: str\n",
    "        Y-axis label (default: \"Production [kWh]\")\n",
    "    ylim: tuple or None\n",
    "        Y-axis limits as (min, max)\n",
    "    save: bool\n",
    "        If True, saves the figure\n",
    "    save_dir: str | Path | None\n",
    "        Directory where the figure will be saved. If None and `save=True`,\n",
    "        pass a directory explicitly (e.g., PLOTS_DIR) or it will fallback to CWD/\"plots\".\n",
    "    base_filename: str | None\n",
    "        Base filename without extension; if None, derived from title.\n",
    "    dpi: int\n",
    "        Resolution for the saved image.\n",
    "    fmt: str\n",
    "        File format for saving (e.g., \"png\", \"pdf\", \"svg\").\n",
    "    \"\"\"\n",
    "    # Prepare data - we want boxplots for each park (column)\n",
    "    data_list = []\n",
    "    labels = []\n",
    "    \n",
    "    for col in mat.columns:\n",
    "        # Get non-NaN values for this park\n",
    "        values = mat[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            data_list.append(values)\n",
    "            labels.append(f\"{extract_park_name_before_pcc(col)}\\n({parse_kwp_from_header(col):.0f} kWp)\")\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(max(12, 0.8 * len(labels)), 6))\n",
    "    \n",
    "    bp = ax.boxplot(data_list, labels=labels, patch_artist=True, showmeans=True,\n",
    "                     meanprops=dict(marker='D', markerfacecolor='red', markeredgecolor='red', markersize=5))\n",
    "    \n",
    "    # Customize box colors\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.set_xlabel(\"PV Parks\", fontsize=12)\n",
    "    \n",
    "    # Rotate x labels if many parks\n",
    "    if len(labels) > 10:\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    \n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Use shared save utility\n",
    "    saved_path = save_figure(\n",
    "        fig=fig,\n",
    "        title_prefix=title,\n",
    "        save=save,\n",
    "        save_dir=save_dir,\n",
    "        base_filename=base_filename,\n",
    "        dpi=dpi,\n",
    "        fmt=fmt,\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return saved_path\n",
    "\n",
    "plot_boxplot(daily_historical, title=\"Daily Boxplot\", save=True, save_dir=PLOTS_DIR / \"weekly_analysis\", \n",
    "             base_filename=\"daily_boxplot\", dpi=180, fmt=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c33aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df: pd.DataFrame, multiplier: float = 1.5) -> dict:\n",
    "    \"\"\"\n",
    "    Detect outliers using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe to analyze\n",
    "    multiplier: float\n",
    "        IQR multiplier for outlier detection (default: 1.5)\n",
    "        - 1.5 is standard for outliers\n",
    "        - 3.0 is often used for extreme outliers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'outlier_counts': Series with count of outliers per column\n",
    "        - 'outlier_pcts': Series with percentage of outliers per column\n",
    "        - 'outlier_mask': DataFrame with True for outliers, False otherwise\n",
    "        - 'bounds': DataFrame with lower_bound and upper_bound per column\n",
    "        - 'summary': DataFrame with statistics for each column\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'outlier_counts': {},\n",
    "        'outlier_pcts': {},\n",
    "        'outlier_mask': pd.DataFrame(index=df.index),\n",
    "        'bounds': [],\n",
    "        'summary': []\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Skip columns with insufficient non-NaN values\n",
    "        non_nan_values = df[col].dropna()\n",
    "        if len(non_nan_values) < 4:\n",
    "            results['outlier_counts'][col] = 0\n",
    "            results['outlier_pcts'][col] = 0.0\n",
    "            results['outlier_mask'][col] = False\n",
    "            results['bounds'].append({\n",
    "                'column': col,\n",
    "                'lower_bound': np.nan,\n",
    "                'upper_bound': np.nan,\n",
    "                'Q1': np.nan,\n",
    "                'Q3': np.nan,\n",
    "                'IQR': np.nan\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Calculate quartiles and IQR\n",
    "        Q1 = non_nan_values.quantile(0.25)\n",
    "        Q3 = non_nan_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Calculate bounds\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        # Detect outliers\n",
    "        outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        outlier_count = outlier_mask.sum()\n",
    "        outlier_pct = (outlier_count / len(non_nan_values)) * 100 if len(non_nan_values) > 0 else 0\n",
    "        \n",
    "        results['outlier_counts'][col] = outlier_count\n",
    "        results['outlier_pcts'][col] = outlier_pct\n",
    "        results['outlier_mask'][col] = outlier_mask\n",
    "        \n",
    "        results['bounds'].append({\n",
    "            'column': col,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR\n",
    "        })\n",
    "        \n",
    "        results['summary'].append({\n",
    "            'column': col,\n",
    "            'total_values': len(df[col]),\n",
    "            'non_nan_values': len(non_nan_values),\n",
    "            'outlier_count': outlier_count,\n",
    "            'outlier_pct': outlier_pct,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "        })\n",
    "    \n",
    "    results['outlier_counts'] = pd.Series(results['outlier_counts'])\n",
    "    results['outlier_pcts'] = pd.Series(results['outlier_pcts'])\n",
    "    results['bounds'] = pd.DataFrame(results['bounds'])\n",
    "    results['summary'] = pd.DataFrame(results['summary'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Detect outliers in the daily dataframe\n",
    "outliers = detect_outliers_iqr(daily_historical, multiplier=1.5)\n",
    "\n",
    "print(\"=== Outlier Detection Summary (IQR method) ===\")\n",
    "print(f\"Total columns analyzed: {len(daily_historical.columns)}\")\n",
    "print(f\"Columns with outliers: {(outliers['outlier_counts'] > 0).sum()}\")\n",
    "print(f\"\\nTop 10 columns with most outliers:\")\n",
    "top_outliers = outliers['summary'].sort_values('outlier_count', ascending=False).head(10)\n",
    "print(top_outliers[['column', 'outlier_count', 'outlier_pct', 'lower_bound', 'upper_bound']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outliers(outliers_result: dict, title_suffix: str = \"IQR Method\"):\n",
    "    \"\"\"\n",
    "    Visualize outlier detection results with bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    outliers_result: dict\n",
    "        Result dictionary from detect_outliers_iqr function\n",
    "    title_suffix: str\n",
    "        Suffix to add to plot titles (default: \"IQR Method\")\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar plot of outlier counts\n",
    "    sorted_counts = outliers_result['outlier_counts'].sort_values(ascending=False)\n",
    "    ax1.bar(range(len(sorted_counts)), sorted_counts.values, color='crimson', alpha=0.7)\n",
    "    ax1.set_xticks(range(len(sorted_counts)))\n",
    "    ax1.set_xticklabels([extract_park_name_before_pcc(col) for col in sorted_counts.index], rotation=45, ha='right')\n",
    "    ax1.set_ylabel(\"Outlier Count\")\n",
    "    ax1.set_xlabel(\"PV Parks\")\n",
    "    ax1.set_title(f\"Outlier Count per Column ({title_suffix})\")\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Bar plot of outlier percentages\n",
    "    sorted_pcts = outliers_result['outlier_pcts'].sort_values(ascending=False)\n",
    "    ax2.bar(range(len(sorted_pcts)), sorted_pcts.values, color='orange', alpha=0.7)\n",
    "    ax2.set_xticks(range(len(sorted_pcts)))\n",
    "    ax2.set_xticklabels([extract_park_name_before_pcc(col) for col in sorted_pcts.index], rotation=45, ha='right')\n",
    "    ax2.set_ylabel(\"Outlier Percentage (%)\")\n",
    "    ax2.set_xlabel(\"PV Parks\")\n",
    "    ax2.set_title(f\"Outlier Percentage per Column ({title_suffix})\")\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the outliers\n",
    "visualize_outliers(outliers, title_suffix=\"IQR Method, multiplier=1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE METADATA WITH ACTUAL COORDINATES\n",
    "# Replace this cell with your actual park metadata\n",
    "# Expected columns: park_name, lat, lon, kwp, loss_pct, timezone\n",
    "\n",
    "# OPTION 1: Load from CSV file with actual coordinates\n",
    "# meta = pd.read_csv(WORKSPACE_ROOT / \"data\" / \"park_metadata.csv\")\n",
    "# Required columns: park_name (matching park_cols), lat, lon, kwp, loss_pct, timezone\n",
    "\n",
    "# OPTION 2: Create manually for each park\n",
    "# Example structure:\n",
    "park_cols = daily_historical.columns.tolist()\n",
    "meta_data = {\n",
    "    'park_name': park_cols,\n",
    "    'latitude': [\n",
    "    38.39,\n",
    "    37.61,\n",
    "    38.31,\n",
    "    38.10,\n",
    "    37.94,\n",
    "    38.31,\n",
    "    38.31,\n",
    "    38.31,\n",
    "    38.31,\n",
    "    38.31,\n",
    "    38.02,\n",
    "    37.95,\n",
    "    38.39,\n",
    "    37.94,\n",
    "    37.95,\n",
    "    38.31,\n",
    "    37.94,\n",
    "    38.30,\n",
    "    37.95,\n",
    "    38.29,\n",
    "    37.89,\n",
    "    38.30\n",
    "],  # REPLACE with actual latitudes\n",
    "    'longitude': [\n",
    "    23.47,\n",
    "    21.89,\n",
    "    23.24,\n",
    "    21.65,\n",
    "    21.30,\n",
    "    23.24,\n",
    "    23.24,\n",
    "    23.24,\n",
    "    23.24,\n",
    "    23.24,\n",
    "    21.40,\n",
    "    21.29,\n",
    "    23.47,\n",
    "    21.30,\n",
    "    21.29,\n",
    "    23.24,\n",
    "    21.30,\n",
    "    23.24,\n",
    "    21.29,\n",
    "    23.29,\n",
    "    21.27,\n",
    "    23.28\n",
    "],  # REPLACE with actual longitudes\n",
    "    'kwp': [parse_kwp_from_header(col) for col in park_cols],  # Extracted from column names\n",
    "    'loss_pct': [18.0] * len(park_cols),  # System losses (adjust per park if needed)\n",
    "    'timezone': ['Europe/Athens'] * len(park_cols)\n",
    "}\n",
    "meta = pd.DataFrame(meta_data)\n",
    "\n",
    "# Validate metadata\n",
    "print(f\"Metadata created for {len(meta)} parks\")\n",
    "print(f\"Park columns count: {len(park_cols)}\")\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  - Latitudes: {len(meta['latitude'])} values\")\n",
    "print(f\"  - Longitudes: {len(meta['longitude'])} values\")\n",
    "print(f\"  - All parks matched: {len(meta) == len(park_cols)}\")\n",
    "\n",
    "# Check if all park names match\n",
    "if set(meta['park_name']) != set(park_cols):\n",
    "    missing_in_meta = set(park_cols) - set(meta['park_name'])\n",
    "    extra_in_meta = set(meta['park_name']) - set(park_cols)\n",
    "    if missing_in_meta:\n",
    "        print(f\"\\nâš ï¸  WARNING: Parks missing in metadata: {len(missing_in_meta)}\")\n",
    "        print(f\"   First few: {list(missing_in_meta)[:3]}\")\n",
    "    if extra_in_meta:\n",
    "        print(f\"\\nâš ï¸  WARNING: Extra parks in metadata: {len(extra_in_meta)}\")\n",
    "else:\n",
    "    print(f\"  âœ“ All park names match!\")\n",
    "\n",
    "meta.set_index('park_name', inplace=True)\n",
    "\n",
    "print(f\"\\nSample metadata:\")\n",
    "print(meta.head(10))\n",
    "print(f\"\\nâœ“ Using actual coordinates from Greece\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload the module to pick up the fix\n",
    "# importlib.reload(pvgis_pi_heatmap)\n",
    "# from src.pvgis_pi_heatmap import compute_pi_anomaly\n",
    "\n",
    "# Recompute with the fixed function\n",
    "pi_fixed, score_fixed, flag_fixed = compute_pi_anomaly(\n",
    "    daily_df=daily_historical,\n",
    "    meta=meta,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    pvgis_url=\"https://re.jrc.ec.europa.eu/api/\",\n",
    ")\n",
    "\n",
    "print(\"\\nFixed PI data info:\")\n",
    "print(f\"  Total rows: {len(pi_fixed)}\")\n",
    "print(f\"  Non-NaN rows: {pi_fixed.notna().any(axis=1).sum()}\")\n",
    "print(f\"  First non-NaN date: {pi_fixed[pi_fixed.notna().any(axis=1)].index.min() if pi_fixed.notna().any(axis=1).any() else 'None'}\")\n",
    "print(f\"  Last non-NaN date: {pi_fixed[pi_fixed.notna().any(axis=1)].index.max() if pi_fixed.notna().any(axis=1).any() else 'None'}\")\n",
    "print(f\"\\nData after 2024-12-31:\")\n",
    "pi_2025plus = pi_fixed[pi_fixed.index > '2024-12-31']\n",
    "print(f\"  Rows: {len(pi_2025plus)}\")\n",
    "print(f\"  Non-NaN values: {pi_2025plus.notna().sum().sum()}\")\n",
    "\n",
    "# Update the main variables\n",
    "pi = pi_fixed\n",
    "score = score_fixed\n",
    "flag = flag_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Daily data date range:\")\n",
    "print(f\"  Start: {daily_historical.index.min()}\")\n",
    "print(f\"  End: {daily_historical.index.max()}\")\n",
    "print(f\"  Years: {daily_historical.index.min().year} to {daily_historical.index.max().year}\")\n",
    "print(f\"\\nPI data info:\")\n",
    "print(f\"  Total rows: {len(pi)}\")\n",
    "print(f\"  Non-NaN rows: {pi.notna().any(axis=1).sum()}\")\n",
    "print(f\"  First non-NaN date: {pi[pi.notna().any(axis=1)].index.min() if pi.notna().any(axis=1).any() else 'None'}\")\n",
    "print(f\"  Last non-NaN date: {pi[pi.notna().any(axis=1)].index.max() if pi.notna().any(axis=1).any() else 'None'}\")\n",
    "print(f\"\\nData after 2024-01-01:\")\n",
    "pi_2024plus = pi[pi.index >= '2024-01-01']\n",
    "print(f\"  Rows: {len(pi_2024plus)}\")\n",
    "print(f\"  Non-NaN values: {pi_2024plus.notna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(\n",
    "    mat: pd.DataFrame,\n",
    "    title: str,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    save: bool = False,\n",
    "    save_dir: str | Path | None = None,\n",
    "    base_filename: str | None = None,\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of date x park data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mat: pd.DataFrame\n",
    "        Date x park dataframe with values to plot\n",
    "    title: str\n",
    "        Plot title\n",
    "    vmin: float or None\n",
    "        Minimum value for colormap\n",
    "    vmax: float or None\n",
    "        Maximum value for colormap\n",
    "    start_date: str or pd.Timestamp or None\n",
    "        Start date for filtering (inclusive). Format: 'YYYY-MM-DD' or any pandas-compatible date\n",
    "    end_date: str or pd.Timestamp or None\n",
    "        End date for filtering (inclusive). Format: 'YYYY-MM-DD' or any pandas-compatible date\n",
    "    save: bool\n",
    "        If True, saves the figure\n",
    "    save_dir: str | Path | None\n",
    "        Directory where the figure will be saved. If None and `save=True`,\n",
    "        pass a directory explicitly (e.g., PLOTS_DIR) or it will fallback to CWD/\"plots\".\n",
    "    base_filename: str | None\n",
    "        Base filename without extension; if None, derived from title.\n",
    "    dpi: int\n",
    "        Resolution for the saved image.\n",
    "    fmt: str\n",
    "        File format for saving (e.g., \"png\", \"pdf\", \"svg\").\n",
    "    \"\"\"\n",
    "    # Filter by date range if specified\n",
    "    if start_date is not None or end_date is not None:\n",
    "        if start_date is not None:\n",
    "            start_date = pd.to_datetime(start_date)\n",
    "            mat = mat[mat.index >= start_date]\n",
    "        if end_date is not None:\n",
    "            end_date = pd.to_datetime(end_date)\n",
    "            mat = mat[mat.index <= end_date]\n",
    "        \n",
    "        if len(mat) == 0:\n",
    "            print(f\"Warning: No data found in the specified date range\")\n",
    "            return None\n",
    "    \n",
    "    # mat: date x park -> we plot parks x dates\n",
    "    m = mat.T.copy()\n",
    "\n",
    "    # nicer y labels (park label + kWp)\n",
    "    y = []\n",
    "    for col in m.index:\n",
    "        y.append(f\"{extract_park_name_before_pcc(col)} ({parse_kwp_from_header(col):.0f} kWp)\")\n",
    "    m.index = y\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, max(6, 0.28 * len(m.index))))\n",
    "    im = ax.imshow(m.values, aspect=\"auto\", interpolation=\"nearest\", cmap=\"turbo\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_yticks(np.arange(len(m.index)))\n",
    "    ax.set_yticklabels(m.index, fontsize=10)\n",
    "\n",
    "    dates = pd.to_datetime(m.columns)\n",
    "    step = max(1, len(dates) // 12)\n",
    "    xticks = np.arange(0, len(dates), step)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([d.strftime(\"%Y-%m-%d\") for d in dates[::step]], rotation=45, ha=\"right\")\n",
    "\n",
    "    # Disable grid lines (they can appear as lines over the heatmap)\n",
    "    ax.grid(False)\n",
    "\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.025, pad=0.02)\n",
    "    cbar.ax.set_ylabel(\"KPI\", rotation=90, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Use shared save utility\n",
    "    saved_path = save_figure(\n",
    "        fig=fig,\n",
    "        title_prefix=title,\n",
    "        save=save,\n",
    "        save_dir=save_dir,\n",
    "        base_filename=base_filename,\n",
    "        dpi=dpi,\n",
    "        fmt=fmt,\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return saved_path\n",
    "\n",
    "# Examples showing KPI data\n",
    "plot_heatmap(pi, \"PI_PVGIS = Measured / PVGIS expected\", vmin=0.0, vmax=1.5,\n",
    "             save=True, save_dir=PLOTS_DIR / \"weekly_analysis\", base_filename=\"heatmap_pi\", dpi=180, fmt=\"png\")\n",
    "plot_heatmap(score, \"Robust anomaly score (rolling median/MAD)\", vmin=-6, vmax=6,\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_score\", dpi=180, fmt=\"png\")\n",
    "plot_heatmap(flag, \"Flags (-1 low, 0 ok, +1 high)\", vmin=-1, vmax=1,\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_flag\", dpi=180, fmt=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual generation data with date filtering\n",
    "plot_heatmap(daily_historical, \"Daily Generation [kWh/day]\", vmin=None, vmax=None,\n",
    "             start_date='2026-01-01', end_date='2026-01-31',\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_daily_jan2026\", dpi=180, fmt=\"png\")\n",
    "\n",
    "# Example: Filter heatmap to show only specific date ranges\n",
    "# Show only January 2026\n",
    "plot_heatmap(pi, \"PI_PVGIS - January 2026\", vmin=0.0, vmax=1.5, \n",
    "             start_date='2026-01-01', end_date='2026-01-31',\n",
    "             save=True, save_dir=PLOTS_DIR / \"weekly_analysis\", base_filename=\"heatmap_pi_jan2026\", dpi=180, fmt=\"png\")\n",
    "\n",
    "# Show only 2025 data\n",
    "plot_heatmap(pi, \"PI_PVGIS - Year 2025\", vmin=0.0, vmax=1.5, \n",
    "             start_date='2025-01-01', end_date='2025-12-31',\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_pi_2025\", dpi=180, fmt=\"png\")\n",
    "\n",
    "# Show Q1 2026\n",
    "plot_heatmap(score, \"Anomaly Score - Q1 2026\", vmin=-6, vmax=6,\n",
    "             start_date='2026-01-01', end_date='2026-01-15',\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_score_q1_2026\", dpi=180, fmt=\"png\")\n",
    "\n",
    "# Show data from a specific date onwards\n",
    "plot_heatmap(flag, \"Flags - From June 2025 onwards\", vmin=-1, vmax=1,\n",
    "             start_date='2025-06-01',\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_flag_from_jun2025\", dpi=180, fmt=\"png\")\n",
    "\n",
    "# Show data up to a specific date\n",
    "plot_heatmap(pi, \"PI_PVGIS - Up to March 2025\", vmin=0.0, vmax=1.5,\n",
    "             end_date='2025-03-31',\n",
    "             save=False, save_dir=PLOTS_DIR, base_filename=\"heatmap_pi_until_mar2025\", dpi=180, fmt=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04003799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_month_to_date_by_year(\n",
    "    df: pd.DataFrame,\n",
    "    column: str | None = None,\n",
    "    aggregation: str = 'sum',\n",
    "    current_date: pd.Timestamp | str | None = None,\n",
    ") -> pd.Series | list:\n",
    "    \"\"\"\n",
    "    Analyze month-to-date values for a specific column (or all columns) across all years in the dataset.\n",
    "    \n",
    "    For the current month (e.g., January 16), this function extracts data from the \n",
    "    start of the month (Jan 1) to the current day (Jan 16) for each year in the dataset\n",
    "    and aggregates it according to the specified method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        Date-indexed DataFrame with columns representing different series/parks\n",
    "    column: str | None\n",
    "        Column name to analyze. If None, aggregates across all columns.\n",
    "        If not found, returns list of available columns.\n",
    "    aggregation: str\n",
    "        Aggregation method: 'sum', 'mean', 'avg', 'min', 'max', 'median', 'std', 'count'\n",
    "        Default: 'sum'\n",
    "    current_date: pd.Timestamp | str | None\n",
    "        Reference date (defaults to today). Used to determine which month and day-of-month\n",
    "        to analyze across all years\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series with year as index and aggregated values, or list of columns if column not found\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # Analyze January 1-16 energy generation (sum) for each year for a specific park\n",
    "    result = analyze_month_to_date_by_year(daily_historical, 'Park A [100 kWp]', 'sum')\n",
    "    \n",
    "    # Analyze January 1-16 energy generation (sum) for each year across all parks\n",
    "    result = analyze_month_to_date_by_year(daily_historical, None, 'sum')\n",
    "    \n",
    "    # Analyze January 1-16 average generation for each year  \n",
    "    result = analyze_month_to_date_by_year(daily_historical, 'Park B [200 kWp]', 'mean')\n",
    "    \"\"\"\n",
    "    # Handle None column - aggregate across all columns\n",
    "    if column is None:\n",
    "        # Set current date (default to today)\n",
    "        if current_date is None:\n",
    "            current_date = pd.Timestamp.now()\n",
    "        else:\n",
    "            current_date = pd.to_datetime(current_date)\n",
    "        \n",
    "        # Extract current month and day\n",
    "        current_month = current_date.month\n",
    "        current_day = current_date.day\n",
    "        \n",
    "        # Map aggregation aliases\n",
    "        agg_map = {\n",
    "            'avg': 'mean',\n",
    "            'average': 'mean',\n",
    "        }\n",
    "        aggregation = agg_map.get(aggregation.lower(), aggregation.lower())\n",
    "        \n",
    "        # Validate aggregation method\n",
    "        valid_aggs = ['sum', 'mean', 'min', 'max', 'median', 'std', 'count']\n",
    "        if aggregation not in valid_aggs:\n",
    "            raise ValueError(f\"Invalid aggregation '{aggregation}'. Valid options: {valid_aggs}\")\n",
    "        \n",
    "        # Get all unique years in the dataset\n",
    "        years = df.index.year.unique()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for year in sorted(years):\n",
    "            # Create date range for this year (month-to-date)\n",
    "            start_date = pd.Timestamp(year=year, month=current_month, day=1)\n",
    "            end_date = pd.Timestamp(year=year, month=current_month, day=current_day)\n",
    "            \n",
    "            # Check if this date range exists for this year\n",
    "            if end_date > df.index.max() or start_date < df.index.min():\n",
    "                # Skip years where the date range doesn't exist\n",
    "                continue\n",
    "            \n",
    "            # Filter data for this period across all columns\n",
    "            mask = (df.index >= start_date) & (df.index <= end_date)\n",
    "            period_data = df.loc[mask].dropna(how='all')\n",
    "            \n",
    "            # Skip if no data for this period\n",
    "            if len(period_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Apply aggregation across all columns first, then aggregate the result\n",
    "            if aggregation == 'sum':\n",
    "                # Sum all values across all columns for this period\n",
    "                value = period_data.sum().sum()\n",
    "            elif aggregation == 'mean':\n",
    "                # Mean of all values across all columns for this period\n",
    "                value = period_data.stack().mean()\n",
    "            elif aggregation == 'min':\n",
    "                # Minimum value across all columns for this period\n",
    "                value = period_data.min().min()\n",
    "            elif aggregation == 'max':\n",
    "                # Maximum value across all columns for this period\n",
    "                value = period_data.max().max()\n",
    "            elif aggregation == 'median':\n",
    "                # Median of all values across all columns for this period\n",
    "                value = period_data.stack().median()\n",
    "            elif aggregation == 'std':\n",
    "                # Standard deviation of all values across all columns for this period\n",
    "                value = period_data.stack().std()\n",
    "            elif aggregation == 'count':\n",
    "                # Count of non-null values across all columns for this period\n",
    "                value = period_data.count().sum()\n",
    "            \n",
    "            results[year] = value\n",
    "        \n",
    "        # Create result series\n",
    "        result = pd.Series(results, name=f'{aggregation.capitalize()}')\n",
    "        result.index.name = 'Year'\n",
    "        \n",
    "        # Print summary\n",
    "        month_name = current_date.strftime('%B')\n",
    "        print(f\"\\nðŸ“Š Analysis: {month_name} 1-{current_day} ({aggregation}) for ALL COLUMNS\")\n",
    "        print(f\"   Columns count: {len(df.columns)}\")\n",
    "        print(f\"   Years analyzed: {len(result)}\")\n",
    "        print(f\"   Date range per year: {month_name} 1 - {month_name} {current_day}\")\n",
    "        print(f\"\\n{result.to_string()}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Check if column exists\n",
    "    if column not in df.columns:\n",
    "        print(f\"âŒ Column '{column}' not found in dataframe\")\n",
    "        print(f\"\\nðŸ“‹ Available columns ({len(df.columns)}):\")\n",
    "        return list(df.columns)\n",
    "    \n",
    "    # Set current date (default to today)\n",
    "    if current_date is None:\n",
    "        current_date = pd.Timestamp.now()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(current_date)\n",
    "    \n",
    "    # Extract current month and day\n",
    "    current_month = current_date.month\n",
    "    current_day = current_date.day\n",
    "    \n",
    "    # Map aggregation aliases\n",
    "    agg_map = {\n",
    "        'avg': 'mean',\n",
    "        'average': 'mean',\n",
    "    }\n",
    "    aggregation = agg_map.get(aggregation.lower(), aggregation.lower())\n",
    "    \n",
    "    # Validate aggregation method\n",
    "    valid_aggs = ['sum', 'mean', 'min', 'max', 'median', 'std', 'count']\n",
    "    if aggregation not in valid_aggs:\n",
    "        raise ValueError(f\"Invalid aggregation '{aggregation}'. Valid options: {valid_aggs}\")\n",
    "    \n",
    "    # Get all unique years in the dataset\n",
    "    years = df.index.year.unique()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for year in sorted(years):\n",
    "        # Create date range for this year (month-to-date)\n",
    "        start_date = pd.Timestamp(year=year, month=current_month, day=1)\n",
    "        end_date = pd.Timestamp(year=year, month=current_month, day=current_day)\n",
    "        \n",
    "        # Check if this date range exists for this year\n",
    "        if end_date > df.index.max() or start_date < df.index.min():\n",
    "            # Skip years where the date range doesn't exist\n",
    "            continue\n",
    "        \n",
    "        # Filter data for this period\n",
    "        mask = (df.index >= start_date) & (df.index <= end_date)\n",
    "        period_data = df.loc[mask, column].dropna()\n",
    "        \n",
    "        # Skip if no data for this period\n",
    "        if len(period_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Apply aggregation\n",
    "        if aggregation == 'sum':\n",
    "            value = period_data.sum()\n",
    "        elif aggregation == 'mean':\n",
    "            value = period_data.mean()\n",
    "        elif aggregation == 'min':\n",
    "            value = period_data.min()\n",
    "        elif aggregation == 'max':\n",
    "            value = period_data.max()\n",
    "        elif aggregation == 'median':\n",
    "            value = period_data.median()\n",
    "        elif aggregation == 'std':\n",
    "            value = period_data.std()\n",
    "        elif aggregation == 'count':\n",
    "            value = period_data.count()\n",
    "        \n",
    "        results[year] = value\n",
    "    \n",
    "    # Create result series\n",
    "    result = pd.Series(results, name=f'{aggregation.capitalize()}')\n",
    "    result.index.name = 'Year'\n",
    "    \n",
    "    # Print summary\n",
    "    month_name = current_date.strftime('%B')\n",
    "    print(f\"\\nðŸ“Š Analysis: {month_name} 1-{current_day} ({aggregation}) for '{short_label(column)}'\")\n",
    "    print(f\"   Column: {column}\")\n",
    "    print(f\"   Years analyzed: {len(result)}\")\n",
    "    print(f\"   Date range per year: {month_name} 1 - {month_name} {current_day}\")\n",
    "    print(f\"\\n{result.to_string()}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"=\"*80)\n",
    "print(\"MONTH-TO-DATE ANALYSIS ACROSS YEARS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test: If column doesn't exist, it returns the column list\n",
    "print(\"\\n1. Testing with invalid column:\")\n",
    "columns_list = analyze_month_to_date_by_year(daily_historical, 'NonExistent Column', 'sum')\n",
    "\n",
    "# Test: Analyze January 1-16 sum for the first park\n",
    "if len(daily_historical.columns) > 0:\n",
    "    print(\"\\n2. Analyzing first park (sum):\")\n",
    "    first_park = daily_historical.columns[0]\n",
    "    jan_sum = analyze_month_to_date_by_year(daily_historical, first_park, 'sum')\n",
    "    \n",
    "    print(\"\\n3. Analyzing first park (mean):\")\n",
    "    jan_mean = analyze_month_to_date_by_year(daily_historical, first_park, 'mean')\n",
    "    \n",
    "    print(\"\\n4. Analyzing first park (max):\")\n",
    "    jan_max = analyze_month_to_date_by_year(daily_historical, first_park, 'max')\n",
    "    \n",
    "    # Test: Analyze aggregation across all columns (column=None)\n",
    "    print(\"\\n5. Analyzing ALL columns (sum) - NEW FEATURE:\")\n",
    "    jan_all_sum = analyze_month_to_date_by_year(daily_historical, None, 'sum')\n",
    "    \n",
    "    print(\"\\n6. Analyzing ALL columns (mean) - NEW FEATURE:\")\n",
    "    jan_all_mean = analyze_month_to_date_by_year(daily_historical, None, 'mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2672ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue_from_energy(\n",
    "    energy_sum: pd.Series,\n",
    "    price_per_kwh: float = 0.2,\n",
    "    currency: str = \"EUR\",\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate revenue from energy generation values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    energy_sum: pd.Series\n",
    "        Series with energy values (kWh) indexed by year\n",
    "    price_per_kwh: float\n",
    "        Price per kWh in the specified currency (default: 0.2)\n",
    "    currency: str\n",
    "        Currency code for display (default: \"EUR\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series with revenue values indexed by year\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # Get month-to-date energy sum for each year\n",
    "    energy = analyze_month_to_date_by_year(daily_historical, 'Park A [100 kWp]', 'sum')\n",
    "    \n",
    "    # Calculate revenue at 0.2 EUR/kWh\n",
    "    revenue = calculate_revenue_from_energy(energy, price_per_kwh=0.2)\n",
    "    \"\"\"\n",
    "    revenue = energy_sum * price_per_kwh\n",
    "    revenue.name = f'Revenue ({currency})'\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nðŸ’° Revenue Calculation\")\n",
    "    print(f\"   Price: {price_per_kwh:.4f} {currency}/kWh\")\n",
    "    print(f\"   Years: {len(revenue)}\")\n",
    "    print(f\"\\n{revenue.to_string()}\")\n",
    "    print(f\"\\n   Total revenue: {revenue.sum():,.2f} {currency}\")\n",
    "    print(f\"   Average revenue/year: {revenue.mean():,.2f} {currency}\")\n",
    "    print(f\"   Min revenue: {revenue.min():,.2f} {currency} (Year {revenue.idxmin()})\")\n",
    "    print(f\"   Max revenue: {revenue.max():,.2f} {currency} (Year {revenue.idxmax()})\")\n",
    "    \n",
    "    return revenue\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REVENUE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(daily_historical.columns) > 0:\n",
    "    first_park = daily_historical.columns[0]\n",
    "    \n",
    "    # Get January 1-16 energy sum for each year\n",
    "    print(\"\\n1. Getting month-to-date energy data for all columns:\")\n",
    "    jan_energy_all_sum = analyze_month_to_date_by_year(daily_historical, None, 'sum')\n",
    "    \n",
    "    # Calculate revenue at default price (0.2 EUR/kWh)\n",
    "    print(\"\\n2. Calculating revenue at 0.2 EUR/kWh:\")\n",
    "    revenue_default = calculate_revenue_from_energy(jan_energy_all_sum, price_per_kwh=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e50408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_revenue_by_year(\n",
    "    revenue_series: pd.Series,\n",
    "    title: str = \"Month-to-Date Revenue by Year\",\n",
    "    price_per_kwh: float = 0.2,\n",
    "    currency: str = \"EUR\",\n",
    "    save: bool = False,\n",
    "    save_dir: str | Path | None = None,\n",
    "    base_filename: str | None = None,\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an appealing bar chart visualization of revenue by year.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    revenue_series: pd.Series\n",
    "        Series with revenue values indexed by year\n",
    "    title: str\n",
    "        Chart title\n",
    "    price_per_kwh: float\n",
    "        Price per kWh (for display purposes)\n",
    "    currency: str\n",
    "        Currency code for display (default: \"EUR\")\n",
    "    save: bool\n",
    "        If True, saves the figure\n",
    "    save_dir: str | Path | None\n",
    "        Directory where the figure will be saved\n",
    "    base_filename: str | None\n",
    "        Base filename without extension; if None, derived from title\n",
    "    dpi: int\n",
    "        Resolution for the saved image\n",
    "    fmt: str\n",
    "        File format for saving\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of (fig, saved_path)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7), facecolor='white')\n",
    "    \n",
    "    # Calculate average and statistics\n",
    "    avg_revenue = revenue_series.mean()\n",
    "    \n",
    "    # Create color list - gradient effect with conditional coloring\n",
    "    colors = []\n",
    "    for value in revenue_series.values:\n",
    "        if value >= avg_revenue * 1.1:\n",
    "            colors.append('#27ae60')  # Dark green - well above average\n",
    "        elif value >= avg_revenue:\n",
    "            colors.append('#2ecc71')  # Light green - above average\n",
    "        elif value >= avg_revenue * 0.9:\n",
    "            colors.append('#f39c12')  # Orange - near average\n",
    "        else:\n",
    "            colors.append('#e74c3c')  # Red - below average\n",
    "    \n",
    "    # Create bar chart with gradient effect\n",
    "    bars = ax.bar(\n",
    "        range(len(revenue_series)), \n",
    "        revenue_series.values,\n",
    "        color=colors,\n",
    "        alpha=0.85,\n",
    "        edgecolor='#34495e',\n",
    "        linewidth=2,\n",
    "        width=0.6\n",
    "    )\n",
    "    \n",
    "    # Add value labels on top of bars with formatted numbers\n",
    "    for i, (year, value) in enumerate(zip(revenue_series.index, revenue_series.values)):\n",
    "        label_y = value + (max(revenue_series.values) * 0.02)\n",
    "        ax.text(\n",
    "            i, label_y,\n",
    "            f'{value:,.0f}\\n{currency}',\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.4', facecolor='yellow', alpha=0.3, edgecolor='none')\n",
    "        )\n",
    "    \n",
    "    # Add average line with annotation\n",
    "    ax.axhline(\n",
    "        avg_revenue,\n",
    "        color='#e74c3c',\n",
    "        linestyle='--',\n",
    "        linewidth=2.5,\n",
    "        label=f'Average: {avg_revenue:,.0f} {currency}',\n",
    "        alpha=0.8,\n",
    "        zorder=2\n",
    "    )\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title(\n",
    "        f'{title}\\n({price_per_kwh} {currency}/kWh)',\n",
    "        fontsize=16,\n",
    "        fontweight='bold',\n",
    "        pad=20,\n",
    "        color='#2c3e50'\n",
    "    )\n",
    "    ax.set_xlabel('Year', fontsize=13, fontweight='bold', color='#34495e')\n",
    "    ax.set_ylabel(f'Revenue [{currency}]', fontsize=13, fontweight='bold', color='#34495e')\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    ax.set_xticks(range(len(revenue_series)))\n",
    "    ax.set_xticklabels(revenue_series.index.astype(str), fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Format y-axis with thousands separator\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000):,}k'))\n",
    "    ax.tick_params(axis='y', labelsize=11)\n",
    "    \n",
    "    # Grid styling\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--', linewidth=1, zorder=0)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Add subtle background\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(fontsize=12, loc='upper left', frameon=True, shadow=True, fancybox=True)\n",
    "    \n",
    "    # Remove top and right spines for cleaner look\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_color('#34495e')\n",
    "    ax.spines['bottom'].set_color('#34495e')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if requested\n",
    "    saved_path = save_figure(\n",
    "        fig=fig,\n",
    "        title_prefix=title,\n",
    "        save=save,\n",
    "        save_dir=save_dir,\n",
    "        base_filename=base_filename,\n",
    "        dpi=dpi,\n",
    "        fmt=fmt,\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REVENUE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ’° Year-by-Year Revenue ({price_per_kwh} {currency}/kWh):\")\n",
    "    print(f\"   {revenue_series.to_string()}\\n\")\n",
    "    print(f\"ðŸ“Š Statistical Summary:\")\n",
    "    print(f\"   Total Revenue:        {revenue_series.sum():>18,.2f} {currency}\")\n",
    "    print(f\"   Average:              {revenue_series.mean():>18,.2f} {currency}\")\n",
    "    print(f\"   Median:               {revenue_series.median():>18,.2f} {currency}\")\n",
    "    print(f\"   Std Deviation:        {revenue_series.std():>18,.2f} {currency}\")\n",
    "    print(f\"   Coefficient of Var:   {(revenue_series.std()/revenue_series.mean()*100):>18.2f}%\")\n",
    "    print(f\"   Min (Year {revenue_series.idxmin()}):     {revenue_series.min():>18,.2f} {currency}\")\n",
    "    print(f\"   Max (Year {revenue_series.idxmax()}):     {revenue_series.max():>18,.2f} {currency}\")\n",
    "    print(f\"   Range:                {revenue_series.max() - revenue_series.min():>18,.2f} {currency}\")\n",
    "    \n",
    "    if len(revenue_series) > 1:\n",
    "        yoy_change = ((revenue_series.iloc[-1] - revenue_series.iloc[0]) / revenue_series.iloc[0]) * 100\n",
    "        print(f\"\\nðŸ“ˆ Growth Metrics:\")\n",
    "        print(f\"   Total Change (First â†’ Last): {yoy_change:>12.2f}%\")\n",
    "        print(f\"   First Year: {revenue_series.iloc[0]:>30,.2f} {currency}\")\n",
    "        print(f\"   Last Year:  {revenue_series.iloc[-1]:>30,.2f} {currency}\")\n",
    "    \n",
    "    if saved_path:\n",
    "        print(f\"\\nâœ… Figure saved to: {saved_path}\")\n",
    "    \n",
    "    return fig, saved_path\n",
    "\n",
    "\n",
    "# Create the revenue visualization\n",
    "print(\"\\nðŸŽ¨ Creating Enhanced Revenue Chart...\\n\")\n",
    "fig, saved_path = plot_revenue_by_year(\n",
    "    revenue_default,\n",
    "    title=\"Month-to-Date Revenue by Year All Parks (January 1-16)\",\n",
    "    price_per_kwh=0.2,\n",
    "    currency=\"EUR\",\n",
    "    save=True,\n",
    "    save_dir=PLOTS_DIR / \"weekly_analysis\",\n",
    "    base_filename=\"revenue_by_year\",\n",
    "    dpi=180,\n",
    "    fmt=\"png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_month_to_date_by_column(\n",
    "    df: pd.DataFrame,\n",
    "    aggregation: str = 'sum',\n",
    "    current_date: pd.Timestamp | str | None = None,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return month-to-date aggregated values for each column.\n",
    "    \n",
    "    This computes the aggregation from the first day of the month up to `current_date`\n",
    "    for the year of `current_date` (default: today), returning a Series indexed by\n",
    "    column with one aggregated value per column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Date-indexed DataFrame where each column represents a park/series.\n",
    "    aggregation : str\n",
    "        One of: 'sum', 'mean'/'avg'/'average', 'min', 'max', 'median', 'std', 'count'.\n",
    "    current_date : pd.Timestamp | str | None\n",
    "        The reference date defining the month-to-date window. If None, uses today.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series indexed by column with the month-to-date aggregated value for each column.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    # Sum of kWh for each park for the current month-to-date\n",
    "    # aggregate_month_to_date_by_column(daily_historical, aggregation='sum')\n",
    "    \n",
    "    # Mean kWh for each park month-to-date on a specific date\n",
    "    # aggregate_month_to_date_by_column(daily_historical, aggregation='mean', current_date='2026-01-17')\n",
    "    \"\"\"\n",
    "    # Normalize and validate current_date\n",
    "    if current_date is None:\n",
    "        current_date = pd.Timestamp.now()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(current_date)\n",
    "    \n",
    "    # Ensure datetime index (naive)\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    df = df.copy()\n",
    "    df.index = idx\n",
    "    \n",
    "    # Map aggregation aliases\n",
    "    agg_map = {\n",
    "        'avg': 'mean',\n",
    "        'average': 'mean',\n",
    "    }\n",
    "    agg = agg_map.get(str(aggregation).lower(), str(aggregation).lower())\n",
    "    \n",
    "    valid_aggs = ['sum', 'mean', 'min', 'max', 'median', 'std', 'count']\n",
    "    if agg not in valid_aggs:\n",
    "        raise ValueError(f\"Invalid aggregation '{aggregation}'. Valid options: {valid_aggs}\")\n",
    "    \n",
    "    # Build month-to-date window for the current year\n",
    "    year = int(current_date.year)\n",
    "    month = int(current_date.month)\n",
    "    day = int(current_date.day)\n",
    "    start_date = pd.Timestamp(year=year, month=month, day=1)\n",
    "    end_date = pd.Timestamp(year=year, month=month, day=day)\n",
    "    \n",
    "    # Slice the DataFrame for the period\n",
    "    mask = (df.index >= start_date) & (df.index <= end_date)\n",
    "    period_df = df.loc[mask]\n",
    "    \n",
    "    if period_df.empty:\n",
    "        # Return a Series of NaNs with the same columns if there is no data\n",
    "        return pd.Series([pd.NA] * len(df.columns), index=df.columns, name=f\"MTD {agg.capitalize()}\")\n",
    "    \n",
    "    # Compute aggregation per column\n",
    "    if agg == 'sum':\n",
    "        out = period_df.sum(axis=0, skipna=True)\n",
    "    elif agg == 'mean':\n",
    "        out = period_df.mean(axis=0, skipna=True)\n",
    "    elif agg == 'min':\n",
    "        out = period_df.min(axis=0, skipna=True)\n",
    "    elif agg == 'max':\n",
    "        out = period_df.max(axis=0, skipna=True)\n",
    "    elif agg == 'median':\n",
    "        out = period_df.median(axis=0, skipna=True)\n",
    "    elif agg == 'std':\n",
    "        out = period_df.std(axis=0, skipna=True)\n",
    "    elif agg == 'count':\n",
    "        out = period_df.count(axis=0)\n",
    "    \n",
    "    month_name = current_date.strftime('%B')\n",
    "    out.name = f\"MTD {agg.capitalize()} ({month_name} 1-{day})\"\n",
    "    return out\n",
    "\n",
    "# Quick validation of aggregate_month_to_date_by_column\n",
    "print(\"\\nðŸ”Ž Computing MTD sum per column...\")\n",
    "mtd_sum_by_col = aggregate_month_to_date_by_column(daily_historical, aggregation='sum')\n",
    "print(mtd_sum_by_col.sort_values(ascending=False).head())\n",
    "\n",
    "print(\"\\nðŸ”Ž Computing MTD mean per column...\")\n",
    "mtd_mean_by_col = aggregate_month_to_date_by_column(daily_historical, aggregation='mean')\n",
    "print(mtd_mean_by_col.sort_values(ascending=False).head())\n",
    "\n",
    "print(\"\\n Computing total revenue from MTD sum at 20 EUR/kWh...\")\n",
    "revenue_all_columns = calculate_revenue_from_energy(mtd_mean_by_col, price_per_kwh=20)\n",
    "print(revenue_all_columns.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c72ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mtd_revenue_by_year_grid(\n",
    "    daily_historical_df: pd.DataFrame,\n",
    "    price_per_kwh: float = 0.2,\n",
    "    currency: str = \"EUR\",\n",
    "    current_date: pd.Timestamp | str | None = None,\n",
    "    power_mapping_df: pd.DataFrame | None = None,\n",
    "    power_mapping_path: Path | str | None = None,\n",
    "    ncols: int = 3,\n",
    "    save: bool = True,\n",
    "    save_dir: Path | None = None,\n",
    "    base_filename: str = \"mtd_revenue_by_year_grid\",\n",
    "    dpi: int = 180,\n",
    "    fmt: str = \"png\",\n",
    ") -> Path | None:\n",
    "    \"\"\"\n",
    "    Grid of revenue-by-year charts, one per park, similar to `plot_revenue_by_year`.\n",
    "\n",
    "    For each column (park), computes month-to-date energy per year via\n",
    "    `analyze_month_to_date_by_year(..., aggregation='sum')`, converts to revenue,\n",
    "    and renders a bar chart with average reference line and value annotations.\n",
    "    \n",
    "    Revenue is normalized per kWp: (energy_kwh * price_per_kwh) / power_kwp\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Current date normalization\n",
    "    if current_date is None:\n",
    "        current_date = pd.Timestamp.now()\n",
    "    else:\n",
    "        current_date = pd.Timestamp(current_date)\n",
    "\n",
    "    # Load power mapping if not provided\n",
    "    if power_mapping_df is None:\n",
    "        if power_mapping_path is None:\n",
    "            power_mapping_path = WORKSPACE_ROOT / \"outputs\" / \"park_power_mapping.csv\"\n",
    "        power_mapping_df = pd.read_csv(power_mapping_path, index_col=0)\n",
    "\n",
    "    # Build a dictionary of park -> power_kwp for quick lookup\n",
    "    power_kwp_dict = {}\n",
    "    for col in daily_historical_df.columns:\n",
    "        if col in power_mapping_df.index:\n",
    "            power_kwp_dict[col] = float(power_mapping_df.loc[col, 'power_kwp'])\n",
    "        else:\n",
    "            # Fallback: try to parse from column name\n",
    "            try:\n",
    "                kwp = parse_kwp_from_header(col)\n",
    "                power_kwp_dict[col] = kwp\n",
    "            except Exception:\n",
    "                print(f\"âš ï¸  Warning: Could not find power for {col}, defaulting to 100 kWp\")\n",
    "                power_kwp_dict[col] = 100.0\n",
    "\n",
    "    # Ensure save_dir\n",
    "    if save_dir is None:\n",
    "        save_dir = PLOTS_DIR / \"financial_analysis\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Columns (parks)\n",
    "    parks = list(daily_historical_df.columns)\n",
    "    nparks = len(parks)\n",
    "    nrows = int(np.ceil(nparks / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows,\n",
    "        ncols=ncols,\n",
    "        figsize=(5.5 * ncols, 4.2 * nrows),\n",
    "        constrained_layout=True,\n",
    "        facecolor=\"white\",\n",
    "    )\n",
    "\n",
    "    axes_list = axes.flatten() if hasattr(axes, \"flatten\") else np.ravel(axes)\n",
    "\n",
    "    # Helper for short label\n",
    "    import re as _re\n",
    "    def _short_label(col: str) -> str:\n",
    "        try:\n",
    "            from src.pvgis_pi_heatmap import short_label as _sl\n",
    "            return _sl(col)\n",
    "        except Exception:\n",
    "            m = _re.search(r\"\\[(.*?)\\]\", str(col))\n",
    "            return m.group(1) if m else str(col)\n",
    "\n",
    "    # Build each subplot\n",
    "    for idx, park in enumerate(parks):\n",
    "        ax = axes_list[idx]\n",
    "        # Month-to-date energy per year\n",
    "        mtd_energy = analyze_month_to_date_by_year(\n",
    "            daily_historical_df,\n",
    "            park,\n",
    "            aggregation='sum',\n",
    "            current_date=current_date,\n",
    "        )\n",
    "        # Convert to revenue: (energy * price) / power_kwp\n",
    "        power_kwp = power_kwp_dict.get(park, 100.0)\n",
    "        mtd_revenue = (mtd_energy * price_per_kwh) / power_kwp\n",
    "\n",
    "        # Compute average and colors similar to plot_revenue_by_year\n",
    "        avg_val = float(mtd_revenue.mean()) if len(mtd_revenue) else 0.0\n",
    "        colors = []\n",
    "        for v in mtd_revenue.values:\n",
    "            if v >= 1.10 * avg_val:\n",
    "                colors.append('#27ae60')\n",
    "            elif v >= avg_val:\n",
    "                colors.append('#2ecc71')\n",
    "            elif v >= 0.90 * avg_val:\n",
    "                colors.append('#f39c12')\n",
    "            else:\n",
    "                colors.append('#e74c3c')\n",
    "\n",
    "        # Bar chart\n",
    "        bars = ax.bar(range(len(mtd_revenue)), mtd_revenue.values,\n",
    "                      color=colors, alpha=0.85, edgecolor='#34495e', linewidth=1.5, width=0.6)\n",
    "\n",
    "        # Value labels\n",
    "        if len(mtd_revenue):\n",
    "            ymax = float(max(mtd_revenue.values))\n",
    "        else:\n",
    "            ymax = 0.0\n",
    "        for i, (year, value) in enumerate(zip(mtd_revenue.index, mtd_revenue.values)):\n",
    "            label_y = value + (ymax * 0.02)\n",
    "            ax.text(i, label_y, f\"{value:,.0f}\\n{currency}/kWp\", ha='center', va='bottom', fontsize=9,\n",
    "                    fontweight='bold', bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3, edgecolor='none'))\n",
    "\n",
    "        # Average line\n",
    "        if len(mtd_revenue):\n",
    "            ax.axhline(avg_val, color='#e74c3c', linestyle='--', linewidth=2, alpha=0.8,\n",
    "                       label=f'Average: {avg_val:,.0f} {currency}/kWp')\n",
    "\n",
    "        # Axis styling\n",
    "        ax.set_xticks(range(len(mtd_revenue)))\n",
    "        ax.set_xticklabels(mtd_revenue.index.astype(str), fontsize=10, fontweight='bold', rotation=45)\n",
    "        ax.set_ylabel(f'Revenue per kWp [{currency}/kWp]', fontsize=10, fontweight='bold', color='#34495e')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--', linewidth=1)\n",
    "        ax.set_facecolor('#f8f9fa')\n",
    "        ax.legend(fontsize=9, loc='upper left', frameon=True, shadow=True, fancybox=True)\n",
    "\n",
    "        # Title per park\n",
    "        ax.set_title(f\"{extract_park_name_before_pcc(park)}\\nMonth-to-Date Revenue per kWp\", fontsize=11, fontweight='bold', color='#2c3e50')\n",
    "\n",
    "        # Remove top/right spines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_color('#34495e')\n",
    "        ax.spines['bottom'].set_color('#34495e')\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(nparks, len(axes_list)):\n",
    "        axes_list[j].axis('off')\n",
    "\n",
    "    month_name = current_date.strftime('%B %Y')\n",
    "    # Add spacing between subplots and title\n",
    "    plt.subplots_adjust(top=0.93, hspace=0.35, wspace=0.3)\n",
    "    \n",
    "    fig.suptitle(f\"Month-to-Date Revenue per kWp by Year â€” All Parks ({month_name})\", fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "    from src.utils import save_figure\n",
    "    saved_path = save_figure(fig, title_prefix=\"MTD Revenue per kWp by Year Grid\", save=save, save_dir=save_dir,\n",
    "                             base_filename=base_filename, dpi=dpi, fmt=fmt)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    return saved_path\n",
    "\n",
    "# Validate: Plot per-park revenue by year grid\n",
    "current_date = pd.Timestamp.now()\n",
    "saved_mtd_year_grid = plot_mtd_revenue_by_year_grid(\n",
    "    daily_historical,\n",
    "    price_per_kwh=0.2,\n",
    "    currency=\"EUR\",\n",
    "    current_date=current_date,\n",
    "    ncols=3,\n",
    "    save=True,\n",
    "    save_dir=PLOTS_DIR / \"weekly_analysis\",\n",
    "    base_filename=\"mtd_revenue_by_year_grid\",\n",
    "    dpi=180,\n",
    "    fmt=\"png\",\n",
    ")\n",
    "print(f\"\\nðŸ“ Saved MTD by year grid to: {saved_mtd_year_grid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_economic_analysis_dashboard(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    price_per_kwh: float = 0.2,\n",
    "    currency: str = \"EUR\",\n",
    "    figsize_main: tuple = (20, 24),\n",
    "    dpi: int = 150,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a comprehensive economic analysis dashboard with multiple visualizations.\n",
    "    \n",
    "    Includes:\n",
    "    - Time series of monthly energy and revenue\n",
    "    - Year-over-year comparison (bar charts)\n",
    "    - Monthly aggregation to identify seasonal patterns\n",
    "    - Correlation heatmap between years\n",
    "    - Autocorrelation analysis (ACF/PACF)\n",
    "    - Growth rates and year-on-year changes\n",
    "    - Distribution analysis\n",
    "    - Rolling statistics (mean, std)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        Date-indexed DataFrame with energy data\n",
    "    column: str\n",
    "        Column name to analyze\n",
    "    price_per_kwh: float\n",
    "        Energy price in currency units (default: 0.2 EUR/kWh)\n",
    "    currency: str\n",
    "        Currency code for display (default: \"EUR\")\n",
    "    figsize_main: tuple\n",
    "        Figure size for main dashboard (default: (20, 24))\n",
    "    dpi: int\n",
    "        Resolution for the dashboard (default: 150)\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    if column not in df.columns:\n",
    "        print(f\"âŒ Column '{column}' not found\")\n",
    "        return\n",
    "    \n",
    "    # Get daily data\n",
    "    daily_series = df[column].dropna()\n",
    "    \n",
    "    # Resample to monthly data\n",
    "    monthly_data = daily_series.resample('MS').sum()\n",
    "    monthly_data.index = monthly_data.index.to_period('M')\n",
    "    \n",
    "    # Calculate revenue\n",
    "    monthly_revenue = monthly_data * price_per_kwh\n",
    "    \n",
    "    # Create the dashboard\n",
    "    fig = plt.figure(figsize=figsize_main, facecolor='white')\n",
    "    gs = fig.add_gridspec(6, 3, hspace=0.35, wspace=0.3, top=0.96)\n",
    "    \n",
    "    print(f\"ðŸ“Š Creating Economic Analysis Dashboard for {short_label(column)}\")\n",
    "    print(f\"   Data range: {daily_series.index.min()} to {daily_series.index.max()}\")\n",
    "    print(f\"   Monthly observations: {len(monthly_data)}\")\n",
    "    \n",
    "    # ========== 1. Time Series of Monthly Energy ==========\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    ax1.plot(range(len(monthly_data)), monthly_data.values, 'o-', linewidth=2.5, \n",
    "             markersize=6, color='#2E86AB', label='Monthly Generation')\n",
    "    ax1.fill_between(range(len(monthly_data)), monthly_data.values, alpha=0.3, color='#2E86AB')\n",
    "    ax1.set_title('Monthly Energy Generation - Time Series', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Month Index')\n",
    "    ax1.set_ylabel('Energy [kWh]')\n",
    "    ax1.grid(alpha=0.3, linestyle='--')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # ========== 2. Rolling Statistics ==========\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    rolling_mean = monthly_data.rolling(window=3).mean()\n",
    "    rolling_std = monthly_data.rolling(window=3).std()\n",
    "    ax2.plot(range(len(monthly_data)), monthly_data.values, 'o-', alpha=0.5, label='Actual')\n",
    "    ax2.plot(range(len(rolling_mean)), rolling_mean.values, 's-', linewidth=2, color='red', label='3-month MA')\n",
    "    ax2.fill_between(range(len(rolling_mean)), \n",
    "                     rolling_mean.values - rolling_std.values,\n",
    "                     rolling_mean.values + rolling_std.values,\n",
    "                     alpha=0.2, color='red')\n",
    "    ax2.set_title('Rolling Mean & Std Dev', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Energy [kWh]')\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.grid(alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ========== 3. Monthly Revenue Time Series ==========\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    colors = ['green' if v > monthly_revenue.mean() else 'coral' for v in monthly_revenue.values]\n",
    "    ax3.bar(range(len(monthly_revenue)), monthly_revenue.values, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    ax3.axhline(monthly_revenue.mean(), color='red', linestyle='--', linewidth=2, label=f'Average: {monthly_revenue.mean():,.0f}')\n",
    "    ax3.set_title(f'Monthly Revenue - Time Series ({currency}/month)', fontsize=13, fontweight='bold')\n",
    "    ax3.set_xlabel('Month Index')\n",
    "    ax3.set_ylabel(f'Revenue [{currency}]')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 4. Revenue Distribution ==========\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    ax4.hist(monthly_revenue.values, bins=15, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(monthly_revenue.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax4.axvline(monthly_revenue.median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "    ax4.set_title('Revenue Distribution', fontsize=13, fontweight='bold')\n",
    "    ax4.set_xlabel(f'Revenue [{currency}]')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend()\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 5. Month-to-Date Analysis by Year ==========\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    monthly_by_year = daily_series.groupby(daily_series.index.month).sum()\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    ax5.bar(range(len(monthly_by_year)), monthly_by_year.values, color='#A23B72', alpha=0.7, edgecolor='black')\n",
    "    ax5.set_xticks(range(12))\n",
    "    ax5.set_xticklabels(months, rotation=45, ha='right')\n",
    "    ax5.set_title('Seasonal Pattern - Energy by Calendar Month', fontsize=13, fontweight='bold')\n",
    "    ax5.set_xlabel('Month')\n",
    "    ax5.set_ylabel('Total Energy [kWh]')\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 6. Seasonal Revenue ==========\n",
    "    ax6 = fig.add_subplot(gs[2, 2])\n",
    "    seasonal_revenue = daily_series.groupby(daily_series.index.month).sum() * price_per_kwh\n",
    "    colors_seasonal = ['gold' if v > seasonal_revenue.mean() else 'lightblue' for v in seasonal_revenue.values]\n",
    "    ax6.bar(range(len(seasonal_revenue)), seasonal_revenue.values, color=colors_seasonal, alpha=0.7, edgecolor='black')\n",
    "    ax6.axhline(seasonal_revenue.mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax6.set_xticks(range(12))\n",
    "    ax6.set_xticklabels(months, rotation=45, ha='right')\n",
    "    ax6.set_title(f'Seasonal Revenue Pattern ({currency})', fontsize=13, fontweight='bold')\n",
    "    ax6.set_ylabel(f'Revenue [{currency}]')\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 7. Year-over-Year Growth Rate ==========\n",
    "    ax7 = fig.add_subplot(gs[3, 0])\n",
    "    yearly_energy = daily_series.groupby(daily_series.index.year).sum()\n",
    "    if len(yearly_energy) > 1:\n",
    "        yoy_growth = yearly_energy.pct_change() * 100\n",
    "        yoy_growth_clean = yoy_growth.dropna()  # Remove NaN from first year\n",
    "        colors_growth = ['green' if x > 0 else 'red' for x in yoy_growth_clean.values]\n",
    "        ax7.bar(range(len(yoy_growth_clean)), yoy_growth_clean.values, color=colors_growth, alpha=0.7, edgecolor='black')\n",
    "        ax7.axhline(0, color='black', linewidth=1)\n",
    "        ax7.set_xticks(range(len(yoy_growth_clean)))\n",
    "        ax7.set_xticklabels(yoy_growth_clean.index, rotation=45, ha='right')\n",
    "        ax7.set_title('Year-over-Year Growth Rate (%)', fontsize=13, fontweight='bold')\n",
    "        ax7.set_ylabel('Growth Rate (%)')\n",
    "        ax7.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 8. Annual Energy by Year ==========\n",
    "    ax8 = fig.add_subplot(gs[3, 1])\n",
    "    ax8.bar(range(len(yearly_energy)), yearly_energy.values, color='#2E86AB', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax8.set_xticks(range(len(yearly_energy)))\n",
    "    ax8.set_xticklabels(yearly_energy.index, rotation=45, ha='right')\n",
    "    ax8.set_title('Annual Energy Generation by Year', fontsize=13, fontweight='bold')\n",
    "    ax8.set_ylabel('Energy [kWh/year]')\n",
    "    ax8.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 9. Annual Revenue by Year ==========\n",
    "    ax9 = fig.add_subplot(gs[3, 2])\n",
    "    yearly_revenue = yearly_energy * price_per_kwh\n",
    "    colors_revenue = ['darkgreen' if v > yearly_revenue.mean() else 'darkred' for v in yearly_revenue.values]\n",
    "    ax9.bar(range(len(yearly_revenue)), yearly_revenue.values, color=colors_revenue, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax9.set_xticks(range(len(yearly_revenue)))\n",
    "    ax9.set_xticklabels(yearly_revenue.index, rotation=45, ha='right')\n",
    "    ax9.set_title(f'Annual Revenue by Year ({currency})', fontsize=13, fontweight='bold')\n",
    "    ax9.set_ylabel(f'Revenue [{currency}/year]')\n",
    "    ax9.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ========== 10. Autocorrelation Function (ACF) ==========\n",
    "    ax10 = fig.add_subplot(gs[4, 0])\n",
    "    try:\n",
    "        plot_acf(monthly_data.dropna(), lags=min(20, len(monthly_data)//2), ax=ax10, title='Autocorrelation (Monthly Data)')\n",
    "        ax10.set_ylabel('ACF')\n",
    "        ax10.grid(alpha=0.3)\n",
    "    except Exception as e:\n",
    "        ax10.text(0.5, 0.5, f'ACF Error: {str(e)[:30]}', ha='center', va='center')\n",
    "    \n",
    "    # ========== 11. Partial Autocorrelation (PACF) ==========\n",
    "    ax11 = fig.add_subplot(gs[4, 1])\n",
    "    try:\n",
    "        plot_pacf(monthly_data.dropna(), lags=min(20, len(monthly_data)//2), ax=ax11, method='ywm', title='Partial Autocorrelation (Monthly)')\n",
    "        ax11.set_ylabel('PACF')\n",
    "        ax11.grid(alpha=0.3)\n",
    "    except Exception as e:\n",
    "        ax11.text(0.5, 0.5, f'PACF Error: {str(e)[:30]}', ha='center', va='center')\n",
    "    \n",
    "    # ========== 12. Monthly Volatility ==========\n",
    "    ax12 = fig.add_subplot(gs[4, 2])\n",
    "    monthly_volatility = monthly_data.rolling(window=3).std()\n",
    "    ax12.plot(range(len(monthly_volatility)), monthly_volatility.values, 'o-', color='#A23B72', linewidth=2, markersize=6)\n",
    "    ax12.fill_between(range(len(monthly_volatility)), monthly_volatility.values, alpha=0.3, color='#A23B72')\n",
    "    ax12.set_title('Rolling Volatility (3-month Std Dev)', fontsize=13, fontweight='bold')\n",
    "    ax12.set_ylabel('Std Dev [kWh]')\n",
    "    ax12.grid(alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ========== 13. Cumulative Energy & Revenue ==========\n",
    "    ax13 = fig.add_subplot(gs[5, 0])\n",
    "    cumsum_energy = monthly_data.cumsum()\n",
    "    ax13.plot(range(len(cumsum_energy)), cumsum_energy.values, 'o-', linewidth=2.5, color='#2E86AB', markersize=5)\n",
    "    ax13.fill_between(range(len(cumsum_energy)), cumsum_energy.values, alpha=0.3, color='#2E86AB')\n",
    "    ax13.set_title('Cumulative Energy Generation', fontsize=13, fontweight='bold')\n",
    "    ax13.set_xlabel('Month Index')\n",
    "    ax13.set_ylabel('Cumulative Energy [kWh]')\n",
    "    ax13.grid(alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ========== 14. Cumulative Revenue ==========\n",
    "    ax14 = fig.add_subplot(gs[5, 1])\n",
    "    cumsum_revenue = monthly_revenue.cumsum()\n",
    "    ax14.plot(range(len(cumsum_revenue)), cumsum_revenue.values, 's-', linewidth=2.5, color='green', markersize=5)\n",
    "    ax14.fill_between(range(len(cumsum_revenue)), cumsum_revenue.values, alpha=0.3, color='green')\n",
    "    ax14.set_title(f'Cumulative Revenue ({currency})', fontsize=13, fontweight='bold')\n",
    "    ax14.set_xlabel('Month Index')\n",
    "    ax14.set_ylabel(f'Cumulative Revenue [{currency}]')\n",
    "    ax14.grid(alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ========== 15. Statistical Summary Box ==========\n",
    "    ax15 = fig.add_subplot(gs[5, 2])\n",
    "    ax15.axis('off')\n",
    "    \n",
    "    # Calculate key statistics\n",
    "    stats_text = f\"\"\"\n",
    "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "    â•‘     ECONOMIC SUMMARY STATISTICS    â•‘\n",
    "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "    â•‘ Energy (Monthly)                   â•‘\n",
    "    â•‘  Mean:        {monthly_data.mean():>15,.0f} kWh\n",
    "    â•‘  Median:      {monthly_data.median():>15,.0f} kWh\n",
    "    â•‘  Std Dev:     {monthly_data.std():>15,.0f} kWh\n",
    "    â•‘  Min:         {monthly_data.min():>15,.0f} kWh\n",
    "    â•‘  Max:         {monthly_data.max():>15,.0f} kWh\n",
    "    â•‘                                    â•‘\n",
    "    â•‘ Revenue (Monthly @ {price_per_kwh} {currency}/kWh)  â•‘\n",
    "    â•‘  Mean:        {monthly_revenue.mean():>15,.0f} {currency}\n",
    "    â•‘  Median:      {monthly_revenue.median():>15,.0f} {currency}\n",
    "    â•‘  Std Dev:     {monthly_revenue.std():>15,.0f} {currency}\n",
    "    â•‘  Min:         {monthly_revenue.min():>15,.0f} {currency}\n",
    "    â•‘  Max:         {monthly_revenue.max():>15,.0f} {currency}\n",
    "    â•‘                                    â•‘\n",
    "    â•‘ Aggregated Totals                  â•‘\n",
    "    â•‘  Total Energy: {daily_series.sum():>14,.0f} kWh\n",
    "    â•‘  Total Revenue:{yearly_revenue.sum():>14,.0f} {currency}\n",
    "    â•‘  Annual Avg:  {yearly_revenue.mean():>15,.0f} {currency}\n",
    "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\"\n",
    "    \n",
    "    ax15.text(0.05, 0.95, stats_text, transform=ax15.transAxes, fontsize=10,\n",
    "              verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle(f'Economic Analysis Dashboard - {short_label(column)} ({parse_kwp_from_header(column):.0f} kWp)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED ECONOMIC ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ“Š ENERGY STATISTICS (Monthly)\")\n",
    "    print(f\"   Mean:              {monthly_data.mean():>15,.0f} kWh\")\n",
    "    print(f\"   Median:            {monthly_data.median():>15,.0f} kWh\")\n",
    "    print(f\"   Std Deviation:     {monthly_data.std():>15,.0f} kWh\")\n",
    "    print(f\"   Coefficient of Variation: {(monthly_data.std()/monthly_data.mean()*100):>6.2f}%\")\n",
    "    print(f\"   Min/Max Ratio:     {(monthly_data.min()/monthly_data.max()):>15.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’° REVENUE STATISTICS (Monthly @ {price_per_kwh} {currency}/kWh)\")\n",
    "    print(f\"   Mean:              {monthly_revenue.mean():>15,.2f} {currency}\")\n",
    "    print(f\"   Median:            {monthly_revenue.median():>15,.2f} {currency}\")\n",
    "    print(f\"   Std Deviation:     {monthly_revenue.std():>15,.2f} {currency}\")\n",
    "    print(f\"   Coefficient of Variation: {(monthly_revenue.std()/monthly_revenue.mean()*100):>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ ANNUAL METRICS\")\n",
    "    print(f\"   Total Energy (all years):  {daily_series.sum():>15,.0f} kWh\")\n",
    "    print(f\"   Total Revenue (all years): {yearly_revenue.sum():>15,.2f} {currency}\")\n",
    "    print(f\"   Average Annual Energy:     {yearly_energy.mean():>15,.0f} kWh/year\")\n",
    "    print(f\"   Average Annual Revenue:    {yearly_revenue.mean():>15,.2f} {currency}/year\")\n",
    "    \n",
    "    if len(yearly_energy) > 1:\n",
    "        print(f\"\\nðŸ“Š YEAR-OVER-YEAR TRENDS\")\n",
    "        print(f\"   Total YoY Growth:        {((yearly_energy.iloc[-1]/yearly_energy.iloc[0])-1)*100:>14.2f}%\")\n",
    "        print(f\"   Average YoY Change:      {(yearly_energy.pct_change().mean()*100):>14.2f}%\")\n",
    "\n",
    "\n",
    "# Create dashboard for the first park\n",
    "if len(daily_historical.columns) > 0:\n",
    "    first_park = daily_historical.columns[0]\n",
    "    create_economic_analysis_dashboard(daily_historical, first_park, price_per_kwh=0.2, currency=\"EUR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_financial_report_for_all_parks(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    price_per_kwh: float = 0.2,\n",
    "    currency: str = \"EUR\",\n",
    "    report_date: str | pd.Timestamp | None = None,\n",
    "    save_dir: Path | None = None,\n",
    "    dpi: int = 150,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate economic analysis dashboard for a single park in the dataframe and create a markdown report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        Date-indexed DataFrame with energy data; each column represents a park\n",
    "    column: str\n",
    "        Column name (park) to analyze\n",
    "    price_per_kwh: float\n",
    "        Energy price in currency units (default: 0.2)\n",
    "    currency: str\n",
    "        Currency code for display (default: \"EUR\")\n",
    "    report_date: str | pd.Timestamp | None\n",
    "        Report date (defaults to today). Format: 'YYYY-MM-DD'\n",
    "    save_dir: Path | None\n",
    "        Directory where plots will be saved. If None, uses PLOTS_DIR.\n",
    "    dpi: int\n",
    "        Resolution for saved images (default: 150)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Path to the generated report file\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy import stats\n",
    "    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "    from src.utils import sanitize_filename\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Set defaults\n",
    "    if save_dir is None:\n",
    "        save_dir = PLOTS_DIR\n",
    "    \n",
    "    if report_date is None:\n",
    "        report_date = pd.Timestamp.now()\n",
    "    else:\n",
    "        report_date = pd.Timestamp(report_date)\n",
    "    \n",
    "    # Validate column exists\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in dataframe. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Create subdirectory for financial analysis plots\n",
    "    financial_dir = Path(save_dir) / \"financial_analysis\"\n",
    "    financial_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"GENERATING FINANCIAL ANALYSIS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Report Date: {report_date.strftime('%B %d, %Y')}\")\n",
    "    print(f\"Park to analyze: {column}\")\n",
    "    print(f\"Save Directory: {financial_dir}\")\n",
    "    print(f\"Price: {price_per_kwh} {currency}/kWh\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nProcessing: {short_label(column)}\")\n",
    "    try:\n",
    "        # Get data for this column\n",
    "        daily_series = df[column].dropna()\n",
    "        \n",
    "        if len(daily_series) == 0:\n",
    "            raise ValueError(f\"No data available for column '{column}'\")\n",
    "        \n",
    "        # Calculate aggregations\n",
    "        monthly_data = daily_series.resample('MS').sum()\n",
    "        monthly_revenue = monthly_data * price_per_kwh\n",
    "        yearly_energy = daily_series.resample('YS').sum()\n",
    "        yearly_revenue = yearly_energy * price_per_kwh\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 24), facecolor='white')\n",
    "        gs = fig.add_gridspec(6, 3, hspace=0.35, wspace=0.3, top=0.96)\n",
    "        \n",
    "        # 1. Time Series of Monthly Energy\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        ax1.plot(range(len(monthly_data)), monthly_data.values, 'o-', linewidth=2.5, \n",
    "                 markersize=6, color='#2E86AB', label='Monthly Generation')\n",
    "        ax1.fill_between(range(len(monthly_data)), monthly_data.values, alpha=0.3, color='#2E86AB')\n",
    "        ax1.set_title('Monthly Energy Generation - Time Series', fontsize=13, fontweight='bold')\n",
    "        ax1.set_xlabel('Month Index')\n",
    "        ax1.set_ylabel('Energy [kWh]')\n",
    "        ax1.grid(alpha=0.3, linestyle='--')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 2. Rolling Statistics\n",
    "        ax2 = fig.add_subplot(gs[0, 2])\n",
    "        rolling_mean = monthly_data.rolling(window=3).mean()\n",
    "        rolling_std = monthly_data.rolling(window=3).std()\n",
    "        ax2.plot(range(len(monthly_data)), monthly_data.values, 'o-', alpha=0.5, label='Actual')\n",
    "        ax2.plot(range(len(rolling_mean)), rolling_mean.values, 's-', linewidth=2, color='red', label='3-month MA')\n",
    "        ax2.fill_between(range(len(rolling_mean)), \n",
    "                       rolling_mean.values - rolling_std.values,\n",
    "                       rolling_mean.values + rolling_std.values,\n",
    "                       alpha=0.2, color='red')\n",
    "        ax2.set_title('Rolling Mean & Std Dev', fontsize=13, fontweight='bold')\n",
    "        ax2.set_ylabel('Energy [kWh]')\n",
    "        ax2.legend(fontsize=9)\n",
    "        ax2.grid(alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # 3. Monthly Revenue Time Series\n",
    "        ax3 = fig.add_subplot(gs[1, :2])\n",
    "        colors = ['green' if v > monthly_revenue.mean() else 'coral' for v in monthly_revenue.values]\n",
    "        ax3.bar(range(len(monthly_revenue)), monthly_revenue.values, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "        ax3.axhline(monthly_revenue.mean(), color='red', linestyle='--', linewidth=2, label=f'Average: {monthly_revenue.mean():,.0f}')\n",
    "        ax3.set_title(f'Monthly Revenue - Time Series ({currency}/month)', fontsize=13, fontweight='bold')\n",
    "        ax3.set_xlabel('Month Index')\n",
    "        ax3.set_ylabel(f'Revenue [{currency}]')\n",
    "        ax3.legend()\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 4. Revenue Distribution\n",
    "        ax4 = fig.add_subplot(gs[1, 2])\n",
    "        ax4.hist(monthly_revenue.values, bins=15, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(monthly_revenue.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        ax4.axvline(monthly_revenue.median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "        ax4.set_title('Revenue Distribution', fontsize=13, fontweight='bold')\n",
    "        ax4.set_xlabel(f'Revenue [{currency}]')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.legend()\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 5. Seasonal Pattern\n",
    "        ax5 = fig.add_subplot(gs[2, :2])\n",
    "        monthly_by_year = daily_series.groupby(daily_series.index.month).sum()\n",
    "        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        ax5.bar(range(len(monthly_by_year)), monthly_by_year.values, color='#A23B72', alpha=0.7, edgecolor='black')\n",
    "        ax5.set_xticks(range(12))\n",
    "        ax5.set_xticklabels(months, rotation=45, ha='right')\n",
    "        ax5.set_title('Seasonal Pattern - Energy by Calendar Month', fontsize=13, fontweight='bold')\n",
    "        ax5.set_xlabel('Month')\n",
    "        ax5.set_ylabel('Total Energy [kWh]')\n",
    "        ax5.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 6. Seasonal Revenue\n",
    "        ax6 = fig.add_subplot(gs[2, 2])\n",
    "        seasonal_revenue = daily_series.groupby(daily_series.index.month).sum() * price_per_kwh\n",
    "        colors_seasonal = ['gold' if v > seasonal_revenue.mean() else 'lightblue' for v in seasonal_revenue.values]\n",
    "        ax6.bar(range(len(seasonal_revenue)), seasonal_revenue.values, color=colors_seasonal, alpha=0.7, edgecolor='black')\n",
    "        ax6.axhline(seasonal_revenue.mean(), color='red', linestyle='--', linewidth=2)\n",
    "        ax6.set_xticks(range(12))\n",
    "        ax6.set_xticklabels(months, rotation=45, ha='right')\n",
    "        ax6.set_title(f'Seasonal Revenue Pattern ({currency})', fontsize=13, fontweight='bold')\n",
    "        ax6.set_ylabel(f'Revenue [{currency}]')\n",
    "        ax6.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 7. Year-over-Year Growth Rate\n",
    "        ax7 = fig.add_subplot(gs[3, 0])\n",
    "        if len(yearly_energy) > 1:\n",
    "            yoy_growth = yearly_energy.pct_change() * 100\n",
    "            yoy_growth_clean = yoy_growth.dropna()\n",
    "            colors_growth = ['green' if x > 0 else 'red' for x in yoy_growth_clean.values]\n",
    "            ax7.bar(range(len(yoy_growth_clean)), yoy_growth_clean.values, color=colors_growth, alpha=0.7, edgecolor='black')\n",
    "            ax7.axhline(0, color='black', linewidth=1)\n",
    "            ax7.set_xticks(range(len(yoy_growth_clean)))\n",
    "            ax7.set_xticklabels(yoy_growth_clean.index, rotation=45, ha='right')\n",
    "            ax7.set_title('Year-over-Year Growth Rate (%)', fontsize=13, fontweight='bold')\n",
    "            ax7.set_ylabel('Growth Rate (%)')\n",
    "            ax7.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 8. Annual Energy by Year\n",
    "        ax8 = fig.add_subplot(gs[3, 1])\n",
    "        ax8.bar(range(len(yearly_energy)), yearly_energy.values, color='#2E86AB', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax8.set_xticks(range(len(yearly_energy)))\n",
    "        ax8.set_xticklabels(yearly_energy.index, rotation=45, ha='right')\n",
    "        ax8.set_title('Annual Energy Generation by Year', fontsize=13, fontweight='bold')\n",
    "        ax8.set_ylabel('Energy [kWh/year]')\n",
    "        ax8.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 9. Annual Revenue by Year\n",
    "        ax9 = fig.add_subplot(gs[3, 2])\n",
    "        colors_revenue = ['darkgreen' if v > yearly_revenue.mean() else 'darkred' for v in yearly_revenue.values]\n",
    "        ax9.bar(range(len(yearly_revenue)), yearly_revenue.values, color=colors_revenue, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax9.set_xticks(range(len(yearly_revenue)))\n",
    "        ax9.set_xticklabels(yearly_revenue.index, rotation=45, ha='right')\n",
    "        ax9.set_title(f'Annual Revenue by Year ({currency})', fontsize=13, fontweight='bold')\n",
    "        ax9.set_ylabel(f'Revenue [{currency}/year]')\n",
    "        ax9.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 10. Autocorrelation Function (ACF)\n",
    "        ax10 = fig.add_subplot(gs[4, 0])\n",
    "        try:\n",
    "            plot_acf(monthly_data.dropna(), lags=min(20, len(monthly_data)//2), ax=ax10, title='Autocorrelation (Monthly Data)')\n",
    "            ax10.set_ylabel('ACF')\n",
    "            ax10.grid(alpha=0.3)\n",
    "        except Exception as e:\n",
    "            ax10.text(0.5, 0.5, f'ACF Error', ha='center', va='center')\n",
    "        \n",
    "        # 11. Partial Autocorrelation (PACF)\n",
    "        ax11 = fig.add_subplot(gs[4, 1])\n",
    "        try:\n",
    "            plot_pacf(monthly_data.dropna(), lags=min(20, len(monthly_data)//2), ax=ax11, method='ywm', title='Partial Autocorrelation (Monthly)')\n",
    "            ax11.set_ylabel('PACF')\n",
    "            ax11.grid(alpha=0.3)\n",
    "        except Exception as e:\n",
    "            ax11.text(0.5, 0.5, f'PACF Error', ha='center', va='center')\n",
    "        \n",
    "        # 12. Monthly Volatility\n",
    "        ax12 = fig.add_subplot(gs[4, 2])\n",
    "        monthly_volatility = monthly_data.rolling(window=3).std()\n",
    "        ax12.plot(range(len(monthly_volatility)), monthly_volatility.values, 'o-', color='#A23B72', linewidth=2, markersize=6)\n",
    "        ax12.fill_between(range(len(monthly_volatility)), monthly_volatility.values, alpha=0.3, color='#A23B72')\n",
    "        ax12.set_title('Rolling Volatility (3-month Std Dev)', fontsize=13, fontweight='bold')\n",
    "        ax12.set_ylabel('Std Dev [kWh]')\n",
    "        ax12.grid(alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # 13. Cumulative Energy\n",
    "        ax13 = fig.add_subplot(gs[5, 0])\n",
    "        cumsum_energy = monthly_data.cumsum()\n",
    "        ax13.plot(range(len(cumsum_energy)), cumsum_energy.values, 'o-', linewidth=2.5, color='#2E86AB', markersize=5)\n",
    "        ax13.fill_between(range(len(cumsum_energy)), cumsum_energy.values, alpha=0.3, color='#2E86AB')\n",
    "        ax13.set_title('Cumulative Energy Generation', fontsize=13, fontweight='bold')\n",
    "        ax13.set_xlabel('Month Index')\n",
    "        ax13.set_ylabel('Cumulative Energy [kWh]')\n",
    "        ax13.grid(alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # 14. Cumulative Revenue\n",
    "        ax14 = fig.add_subplot(gs[5, 1])\n",
    "        cumsum_revenue = monthly_revenue.cumsum()\n",
    "        ax14.plot(range(len(cumsum_revenue)), cumsum_revenue.values, 's-', linewidth=2.5, color='green', markersize=5)\n",
    "        ax14.fill_between(range(len(cumsum_revenue)), cumsum_revenue.values, alpha=0.3, color='green')\n",
    "        ax14.set_title(f'Cumulative Revenue ({currency})', fontsize=13, fontweight='bold')\n",
    "        ax14.set_xlabel('Month Index')\n",
    "        ax14.set_ylabel(f'Cumulative Revenue [{currency}]')\n",
    "        ax14.grid(alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # 15. Statistical Summary Box\n",
    "        ax15 = fig.add_subplot(gs[5, 2])\n",
    "        ax15.axis('off')\n",
    "        stats_text = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     ECONOMIC SUMMARY STATISTICS    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ Energy (Monthly)                   â•‘\n",
    "â•‘  Mean:        {monthly_data.mean():>15,.0f} kWh\n",
    "â•‘  Median:      {monthly_data.median():>15,.0f} kWh\n",
    "â•‘  Std Dev:     {monthly_data.std():>15,.0f} kWh\n",
    "â•‘  Min:         {monthly_data.min():>15,.0f} kWh\n",
    "â•‘  Max:         {monthly_data.max():>15,.0f} kWh\n",
    "â•‘                                    â•‘\n",
    "â•‘ Revenue (Monthly @ {price_per_kwh} {currency}/kWh)  â•‘\n",
    "â•‘  Mean:        {monthly_revenue.mean():>15,.0f} {currency}\n",
    "â•‘  Median:      {monthly_revenue.median():>15,.0f} {currency}\n",
    "â•‘  Std Dev:     {monthly_revenue.std():>15,.0f} {currency}\n",
    "â•‘  Min:         {monthly_revenue.min():>15,.0f} {currency}\n",
    "â•‘  Max:         {monthly_revenue.max():>15,.0f} {currency}\n",
    "â•‘                                    â•‘\n",
    "â•‘ Aggregated Totals                  â•‘\n",
    "â•‘  Total Energy: {daily_series.sum():>14,.0f} kWh\n",
    "â•‘  Total Revenue:{yearly_revenue.sum():>14,.0f} {currency}\n",
    "â•‘  Annual Avg:  {yearly_revenue.mean():>15,.0f} {currency}\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        \"\"\"\n",
    "        ax15.text(0.05, 0.95, stats_text, transform=ax15.transAxes, fontsize=10,\n",
    "                 verticalalignment='top', fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        plt.suptitle(f'Economic Analysis Dashboard - {short_label(column)} ({parse_kwp_from_header(column):.0f} kWp)', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        # Save figure\n",
    "        safe_name = sanitize_filename(column)\n",
    "        plot_path = financial_dir / f\"financial_analysis_{safe_name}.png\"\n",
    "        fig.savefig(plot_path, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"  âœ“ Saved: {plot_path.name}\")\n",
    "        \n",
    "        # Generate Markdown Report\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"GENERATING MARKDOWN REPORT\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        report_filename = f\"report_weekly_financial_{report_date.strftime('%Y-%m-%d')}.md\"\n",
    "        report_path = WORKSPACE_ROOT / \"docs\" / report_filename\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            # Header\n",
    "            f.write(f\"# Financial Analysis Report - {short_label(column)}\\n\")\n",
    "            f.write(f\"**Report Date:** {report_date.strftime('%B %d, %Y')}\\n\\n\")\n",
    "            f.write(f\"**Park:** {column}\\n\")\n",
    "            f.write(f\"**Capacity:** {parse_kwp_from_header(column):.0f} kWp\\n\")\n",
    "            f.write(f\"**Pricing:** {price_per_kwh} {currency}/kWh\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "            \n",
    "            # Executive Summary\n",
    "            f.write(\"## Executive Summary\\n\\n\")\n",
    "            f.write(f\"This report presents a comprehensive financial analysis of {short_label(column)} \")\n",
    "            f.write(\"with detailed economic metrics, revenue projections, and performance trends.\\n\\n\")\n",
    "            \n",
    "            # Overall Statistics\n",
    "            f.write(\"### Overview\\n\\n\")\n",
    "            f.write(f\"- **Park Name:** {column}\\n\")\n",
    "            f.write(f\"- **Data Range:** {daily_series.index.min().strftime('%Y-%m-%d')} to {daily_series.index.max().strftime('%Y-%m-%d')}\\n\")\n",
    "            f.write(f\"- **Total Energy Generated:** {daily_series.sum():,.0f} kWh\\n\")\n",
    "            f.write(f\"- **Total Revenue:** {yearly_revenue.sum():,.2f} {currency}\\n\")\n",
    "            f.write(f\"- **Energy Price:** {price_per_kwh} {currency}/kWh\\n\\n\")\n",
    "            \n",
    "            # Plot\n",
    "            rel_path = Path(\"..\") / \"plots\" / \"financial_analysis\" / plot_path.name\n",
    "            f.write(f\"![Economic Analysis - {short_label(column)}]({rel_path})\\n\\n\")\n",
    "            \n",
    "            # Key Metrics\n",
    "            f.write(\"### Key Financial Metrics\\n\\n\")\n",
    "            f.write(\"| Metric | Value |\\n\")\n",
    "            f.write(\"|--------|-------|\\n\")\n",
    "            f.write(f\"| Total Energy | {daily_series.sum():,.0f} kWh |\\n\")\n",
    "            f.write(f\"| Total Revenue | {yearly_revenue.sum():,.2f} {currency} |\\n\")\n",
    "            f.write(f\"| Avg Monthly Energy | {monthly_data.mean():,.0f} kWh |\\n\")\n",
    "            f.write(f\"| Avg Monthly Revenue | {monthly_revenue.mean():,.2f} {currency} |\\n\")\n",
    "            f.write(f\"| Avg Annual Energy | {yearly_energy.mean():,.0f} kWh |\\n\")\n",
    "            f.write(f\"| Avg Annual Revenue | {yearly_revenue.mean():,.2f} {currency} |\\n\\n\")\n",
    "            \n",
    "            # Year-by-Year Revenue\n",
    "            f.write(\"### Annual Revenue Breakdown\\n\\n\")\n",
    "            f.write(\"| Year | Energy (kWh) | Revenue ({}) |\\n\".format(currency))\n",
    "            f.write(\"|------|--------------|-------------|\\n\")\n",
    "            for year in sorted(yearly_energy.index):\n",
    "                energy = yearly_energy.loc[year]\n",
    "                revenue = yearly_revenue.loc[year]\n",
    "                f.write(f\"| {year} | {energy:,.0f} | {revenue:,.2f} |\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Key Observations\n",
    "            f.write(\"### Key Observations\\n\\n\")\n",
    "            \n",
    "            # Best and worst performing years\n",
    "            best_year = yearly_revenue.idxmax()\n",
    "            worst_year = yearly_revenue.idxmin()\n",
    "            best_revenue = yearly_revenue.max()\n",
    "            worst_revenue = yearly_revenue.min()\n",
    "            \n",
    "            f.write(f\"- **Best Performing Year:** {best_year} ({best_revenue:,.2f} {currency})\\n\")\n",
    "            f.write(f\"- **Worst Performing Year:** {worst_year} ({worst_revenue:,.2f} {currency})\\n\")\n",
    "            \n",
    "            # Growth trend\n",
    "            if len(yearly_revenue) > 1:\n",
    "                first_year_rev = yearly_revenue.iloc[0]\n",
    "                last_year_rev = yearly_revenue.iloc[-1]\n",
    "                total_growth = ((last_year_rev - first_year_rev) / first_year_rev) * 100\n",
    "                f.write(f\"- **Overall Growth:** {total_growth:+.2f}% (from {yearly_revenue.index[0]} to {yearly_revenue.index[-1]})\\n\")\n",
    "            \n",
    "            f.write(\"\\n---\\n\\n\")\n",
    "            \n",
    "            # Methodology\n",
    "            f.write(\"## Methodology\\n\\n\")\n",
    "            f.write(\"### Revenue Calculation\\n\")\n",
    "            f.write(f\"- **Revenue = Energy Generation (kWh) Ã— Price ({price_per_kwh} {currency}/kWh)**\\n\")\n",
    "            f.write(\"- Monthly aggregation from daily generation data\\n\")\n",
    "            f.write(\"- Annual totals calculated from daily observations\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Dashboard Components\\n\")\n",
    "            f.write(\"The dashboard includes:\\n\")\n",
    "            f.write(\"1. **Time Series Analysis:** Monthly energy and revenue trends\\n\")\n",
    "            f.write(\"2. **Seasonal Patterns:** Energy generation by calendar month\\n\")\n",
    "            f.write(\"3. **Year-over-Year Analysis:** Annual comparisons and growth rates\\n\")\n",
    "            f.write(\"4. **Statistical Analysis:** ACF/PACF, volatility, distributions\\n\")\n",
    "            f.write(\"5. **Cumulative Metrics:** Total energy and revenue over time\\n\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"## Recommendations\\n\\n\")\n",
    "            f.write(\"1. **Financial Planning:** Use annual averages for budget forecasting\\n\")\n",
    "            f.write(\"2. **Seasonal Adjustment:** Account for monthly variations in cash flow planning\\n\")\n",
    "            f.write(\"3. **Performance Monitoring:** Track deviations from historical averages\\n\")\n",
    "            f.write(\"4. **Growth Opportunities:** Identify underperforming months for optimization\\n\\n\")\n",
    "            \n",
    "            # Footer\n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(f\"*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\")\n",
    "        \n",
    "        print(f\"âœ“ Report saved: {report_path}\")\n",
    "        print(f\"âœ“ Generated financial analysis plot\")\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        return report_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error processing {column}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Generate the financial report for a specific park\n",
    "if len(daily_historical.columns) > 0:\n",
    "    # User selects which park to analyze (update 'first_park' with desired column name)\n",
    "    selected_park = daily_historical.columns[0]  # Change this to select different parks\n",
    "    \n",
    "    report_path = create_financial_report_for_all_parks(\n",
    "        df=daily_historical,\n",
    "        column=selected_park,\n",
    "        price_per_kwh=0.2,\n",
    "        currency=\"EUR\",\n",
    "        report_date=None,  # Uses today's date\n",
    "        save_dir=PLOTS_DIR,\n",
    "        dpi=150,\n",
    "    )\n",
    "    print(f\"\\nðŸ“„ Financial Report: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ded13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_revenue_by_year_grid(\n",
    "    daily_historical_df: pd.DataFrame,\n",
    "    current_date: pd.Timestamp | None = None,\n",
    "    price_per_kwh: float = 0.2,\n",
    "    currency: str = \"EUR\",\n",
    "    ncols: int = 3,\n",
    "    save: bool = True,\n",
    "    save_dir: Path | None = None,\n",
    "    base_filename: str = \"revenue_mtd_grid\",\n",
    "    dpi: int = 150,\n",
    "    fmt: str = \"png\",\n",
    ") -> Path | None:\n",
    "    \"\"\"\n",
    "    Generate a grid of month-to-date revenue charts, one per park.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    daily_historical_df: pd.DataFrame\n",
    "        Date-indexed DataFrame with energy data; columns are parks\n",
    "    current_date: pd.Timestamp | None\n",
    "        Reference date for month-to-date calculation (default: today)\n",
    "    price_per_kwh: float\n",
    "        Energy price in currency units\n",
    "    currency: str\n",
    "        Currency code for display\n",
    "    ncols: int\n",
    "        Number of columns in grid (default: 3)\n",
    "    save: bool\n",
    "        Whether to save the figure\n",
    "    save_dir: Path | None\n",
    "        Directory for saving (uses PLOTS_DIR if None)\n",
    "    base_filename: str\n",
    "        Base filename for saving\n",
    "    dpi: int\n",
    "        Resolution for saved images\n",
    "    fmt: str\n",
    "        Image format ('png', 'jpg', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Path to saved figure, or None if not saved\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = PLOTS_DIR\n",
    "    \n",
    "    if current_date is None:\n",
    "        current_date = pd.Timestamp.now()\n",
    "    else:\n",
    "        current_date = pd.Timestamp(current_date)\n",
    "    \n",
    "    # Get all columns (parks)\n",
    "    parks = daily_historical_df.columns.tolist()\n",
    "    nparks = len(parks)\n",
    "    nrows = int(np.ceil(nparks / ncols))\n",
    "    \n",
    "    # Create grid\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows,\n",
    "        ncols=ncols,\n",
    "        figsize=(5 * ncols, 4 * nrows),\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    if nparks == 1:\n",
    "        axes_list = [axes]\n",
    "    else:\n",
    "        axes_list = axes.flatten() if hasattr(axes, 'flatten') else axes.flat\n",
    "    \n",
    "    # Generate chart for each park\n",
    "    for idx, park in enumerate(parks):\n",
    "        ax = axes_list[idx]\n",
    "        \n",
    "        # Get month-to-date energy by year\n",
    "        mtd_energy = analyze_month_to_date_by_year(\n",
    "            daily_historical_df,\n",
    "            park,\n",
    "            aggregation='sum',\n",
    "            current_date=current_date,\n",
    "        )\n",
    "        \n",
    "        # Convert to revenue\n",
    "        mtd_revenue = mtd_energy * price_per_kwh\n",
    "        \n",
    "        # Plot\n",
    "        avg_revenue = mtd_revenue.mean()\n",
    "        colors = []\n",
    "        for val in mtd_revenue.values:\n",
    "            if val > avg_revenue * 1.1:\n",
    "                colors.append('#2d7f2d')  # Dark green\n",
    "            elif val > avg_revenue:\n",
    "                colors.append('#90ee90')  # Light green\n",
    "            elif val > avg_revenue * 0.9:\n",
    "                colors.append('#ffb84d')  # Orange\n",
    "            else:\n",
    "                colors.append('#cc0000')  # Red\n",
    "        \n",
    "        bars = ax.bar(mtd_revenue.index, mtd_revenue.values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, mtd_revenue.values):\n",
    "            height = bar.get_height()\n",
    "            label_text = f'{val:,.0f}'\n",
    "            bbox_props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray')\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                height,\n",
    "                label_text,\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=9,\n",
    "                fontweight='bold',\n",
    "                bbox=bbox_props,\n",
    "            )\n",
    "        \n",
    "        # Add average line\n",
    "        ax.axhline(avg_revenue, color='navy', linestyle='--', linewidth=2, alpha=0.7, label=f'Avg: {avg_revenue:,.0f}')\n",
    "        \n",
    "        # Labels and title\n",
    "        park_label = extract_park_name_before_pcc(park)\n",
    "        ax.set_title(f'{park_label}\\nMonth-to-Date Revenue', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Year', fontsize=10)\n",
    "        ax.set_ylabel(f'Revenue ({currency})', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66434e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_degradation_with_stl(\n",
    "    series: pd.Series,\n",
    "    apply_log: bool = False,\n",
    "    period: int = 365,\n",
    "    robust: bool = True,\n",
    "    anomaly_threshold: float = -3.0,\n",
    "    min_consecutive_days: int = 2,\n",
    "    return_components: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze PV degradation using STL decomposition with robust anomaly detection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series: pd.Series\n",
    "        Time series data (date-indexed) for a single park\n",
    "    apply_log: bool\n",
    "        Whether to apply log transformation before STL (default: False)\n",
    "    period: int\n",
    "        Seasonal period for STL decomposition (default: 365)\n",
    "    robust: bool\n",
    "        Whether to use robust STL fitting (default: True)\n",
    "    anomaly_threshold: float\n",
    "        Z-score threshold for anomaly detection (default: -3.0)\n",
    "    min_consecutive_days: int\n",
    "        Minimum consecutive days for persistent anomalies (default: 2)\n",
    "    return_components: bool\n",
    "        Whether to return full decomposition components (default: False)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - degradation_rate: Monthly degradation rate (% per month)\n",
    "        - annual_degradation: Annual degradation rate (% per year)\n",
    "        - trend_slope: Linear trend coefficient\n",
    "        - anomaly_count: Total number of anomalous days\n",
    "        - persistent_anomaly_count: Number of persistent anomalies (consecutive days)\n",
    "        - anomaly_flags: Boolean series marking anomalous days\n",
    "        - residual_z_scores: Robust z-scores of residuals\n",
    "        - monthly_medians: Monthly median values with trend\n",
    "        - stl_result: STL decomposition object (if return_components=True)\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.seasonal import STL\n",
    "    from scipy.stats import linregress\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Remove NaN values and ensure series is sorted\n",
    "    series_clean = series.dropna().sort_index()\n",
    "    \n",
    "    if len(series_clean) < period * 2:\n",
    "        raise ValueError(f\"Insufficient data: need at least {period * 2} observations, got {len(series_clean)}\")\n",
    "    \n",
    "    # Apply log transformation if requested\n",
    "    if apply_log:\n",
    "        # Add small constant to avoid log(0)\n",
    "        series_transformed = np.log(series_clean + 1e-6)\n",
    "    else:\n",
    "        series_transformed = series_clean.copy()\n",
    "    \n",
    "    # Perform STL decomposition\n",
    "    stl = STL(series_transformed, period=period, robust=robust)\n",
    "    stl_result = stl.fit()\n",
    "    \n",
    "    # Extract components\n",
    "    trend = stl_result.trend\n",
    "    seasonal = stl_result.seasonal\n",
    "    residual = stl_result.resid\n",
    "    \n",
    "    # Compute robust z-scores using MAD (Median Absolute Deviation)\n",
    "    median_resid = np.median(residual)\n",
    "    mad = np.median(np.abs(residual - median_resid))\n",
    "    \n",
    "    # MAD-based robust z-score (1.4826 is consistency constant for normal distribution)\n",
    "    if mad > 0:\n",
    "        residual_z_scores = (residual - median_resid) / (1.4826 * mad)\n",
    "    else:\n",
    "        residual_z_scores = pd.Series(0, index=residual.index)\n",
    "    \n",
    "    # Flag anomalies based on magnitude\n",
    "    anomaly_flags = residual_z_scores < anomaly_threshold\n",
    "    \n",
    "    # Identify persistent anomalies (consecutive days)\n",
    "    persistent_anomalies = pd.Series(False, index=anomaly_flags.index)\n",
    "    \n",
    "    if min_consecutive_days > 1:\n",
    "        # Find consecutive sequences of anomalies\n",
    "        anomaly_groups = (anomaly_flags != anomaly_flags.shift()).cumsum()\n",
    "        consecutive_counts = anomaly_flags.groupby(anomaly_groups).transform('sum')\n",
    "        persistent_anomalies = anomaly_flags & (consecutive_counts >= min_consecutive_days)\n",
    "    else:\n",
    "        persistent_anomalies = anomaly_flags.copy()\n",
    "    \n",
    "    # Fit robust linear regression to trend component\n",
    "    # Convert dates to numeric (days since start)\n",
    "    days_numeric = (trend.index - trend.index[0]).days.values\n",
    "    \n",
    "    # Use scipy's linregress for robust estimation\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(days_numeric, trend.values)\n",
    "    \n",
    "    # Calculate monthly medians from the trend\n",
    "    monthly_trend = trend.resample('MS').median()\n",
    "    \n",
    "    # Compute degradation rate from trend slope\n",
    "    # Slope is in units per day, convert to percentage per month\n",
    "    mean_value = trend.mean()\n",
    "    if apply_log:\n",
    "        # For log-transformed data, slope already represents relative change\n",
    "        degradation_per_day = slope  # This is already in log space\n",
    "        degradation_per_month = degradation_per_day * 30.44  # Average days per month\n",
    "        annual_degradation = degradation_per_day * 365.25\n",
    "    else:\n",
    "        # For linear data, compute relative degradation\n",
    "        if mean_value != 0:\n",
    "            degradation_per_day = (slope / mean_value)  # Relative change per day\n",
    "            degradation_per_month = degradation_per_day * 30.44 * 100  # % per month\n",
    "            annual_degradation = degradation_per_day * 365.25 * 100  # % per year\n",
    "        else:\n",
    "            degradation_per_month = 0\n",
    "            annual_degradation = 0\n",
    "    \n",
    "    # Fit linear regression to monthly medians for additional validation\n",
    "    if len(monthly_trend) >= 2:\n",
    "        monthly_days = (monthly_trend.index - monthly_trend.index[0]).days.values\n",
    "        monthly_slope, monthly_intercept, _, _, _ = linregress(monthly_days, monthly_trend.values)\n",
    "        \n",
    "        if apply_log:\n",
    "            monthly_degradation_rate = monthly_slope * 30.44\n",
    "        else:\n",
    "            monthly_mean = monthly_trend.mean()\n",
    "            if monthly_mean != 0:\n",
    "                monthly_degradation_rate = (monthly_slope / monthly_mean) * 30.44 * 100\n",
    "            else:\n",
    "                monthly_degradation_rate = 0\n",
    "    else:\n",
    "        monthly_degradation_rate = degradation_per_month\n",
    "    \n",
    "    # Prepare results dictionary\n",
    "    results = {\n",
    "        'degradation_rate': monthly_degradation_rate,\n",
    "        'annual_degradation': annual_degradation if not apply_log else annual_degradation * 100,\n",
    "        'trend_slope': slope,\n",
    "        'trend_intercept': intercept,\n",
    "        'trend_r_squared': r_value**2,\n",
    "        'anomaly_count': anomaly_flags.sum(),\n",
    "        'persistent_anomaly_count': persistent_anomalies.sum(),\n",
    "        'anomaly_percentage': (anomaly_flags.sum() / len(anomaly_flags)) * 100,\n",
    "        'persistent_anomaly_percentage': (persistent_anomalies.sum() / len(persistent_anomalies)) * 100,\n",
    "        'anomaly_flags': anomaly_flags,\n",
    "        'persistent_anomaly_flags': persistent_anomalies,\n",
    "        'residual_z_scores': residual_z_scores,\n",
    "        'monthly_medians': monthly_trend,\n",
    "        'mean_residual_mad': mad,\n",
    "        'data_points': len(series_clean),\n",
    "        'date_range': f\"{series_clean.index.min().date()} to {series_clean.index.max().date()}\",\n",
    "        # Store parameters for reporting\n",
    "        'parameters': {\n",
    "            'apply_log': apply_log,\n",
    "            'period': period,\n",
    "            'robust': robust,\n",
    "            'anomaly_threshold': anomaly_threshold,\n",
    "            'min_consecutive_days': min_consecutive_days,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if return_components:\n",
    "        results['stl_result'] = stl_result\n",
    "        results['trend'] = trend\n",
    "        results['seasonal'] = seasonal\n",
    "        results['residual'] = residual\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if len(daily_historical.columns) > 0:\n",
    "    # Test on first park\n",
    "    test_park = daily_historical.columns[0]\n",
    "    \n",
    "    print(f\"Analyzing degradation for: {short_label(test_park)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Run analysis without log transformation\n",
    "    results = analyze_degradation_with_stl(\n",
    "        series=daily_historical[test_park],\n",
    "        apply_log=False,\n",
    "        period=365,\n",
    "        robust=True,\n",
    "        anomaly_threshold=-3.0,\n",
    "        min_consecutive_days=2,\n",
    "        return_components=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Data Range: {results['date_range']}\")\n",
    "    print(f\"Data Points: {results['data_points']:,}\")\n",
    "    print(f\"\\nDegradation Analysis:\")\n",
    "    print(f\"  Monthly Degradation Rate: {results['degradation_rate']:+.4f}% per month\")\n",
    "    print(f\"  Annual Degradation Rate:  {results['annual_degradation']:+.4f}% per year\")\n",
    "    print(f\"  Trend RÂ²: {results['trend_r_squared']:.4f}\")\n",
    "    print(f\"\\nAnomaly Detection:\")\n",
    "    print(f\"  Total Anomalies: {results['anomaly_count']} ({results['anomaly_percentage']:.2f}%)\")\n",
    "    print(f\"  Persistent Anomalies: {results['persistent_anomaly_count']} ({results['persistent_anomaly_percentage']:.2f}%)\")\n",
    "    print(f\"  Residual MAD: {results['mean_residual_mad']:.2f}\")\n",
    "    \n",
    "    # Visualize the decomposition\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(16, 12), sharex=True)\n",
    "    \n",
    "    # Original data\n",
    "    axes[0].plot(results['stl_result'].observed.index, results['stl_result'].observed.values, \n",
    "                 linewidth=0.8, alpha=0.7, color='black')\n",
    "    axes[0].set_ylabel('Observed', fontsize=11)\n",
    "    axes[0].set_title(f'STL Decomposition with Anomaly Detection - {short_label(test_park)}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Trend with linear fit\n",
    "    axes[1].plot(results['trend'].index, results['trend'].values, \n",
    "                 linewidth=1.5, color='#2E86AB', label='Trend')\n",
    "    days_numeric = (results['trend'].index - results['trend'].index[0]).days.values\n",
    "    trend_fit = results['trend_slope'] * days_numeric + results['trend_intercept']\n",
    "    axes[1].plot(results['trend'].index, trend_fit, '--', \n",
    "                 linewidth=2, color='red', alpha=0.8, \n",
    "                 label=f'Linear Fit (slope={results[\"trend_slope\"]:.4e})')\n",
    "    axes[1].set_ylabel('Trend', fontsize=11)\n",
    "    axes[1].legend(loc='best', fontsize=9)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(results['seasonal'].index, results['seasonal'].values, \n",
    "                 linewidth=0.8, color='#A23B72', alpha=0.7)\n",
    "    axes[2].set_ylabel('Seasonal', fontsize=11)\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    # Residuals with anomaly highlighting\n",
    "    axes[3].plot(results['residual'].index, results['residual'].values, \n",
    "                 linewidth=0.6, color='gray', alpha=0.5, label='Normal')\n",
    "    # Highlight all anomalies\n",
    "    anomaly_dates = results['anomaly_flags'][results['anomaly_flags']].index\n",
    "    if len(anomaly_dates) > 0:\n",
    "        axes[3].scatter(anomaly_dates, results['residual'].loc[anomaly_dates], \n",
    "                       color='orange', s=15, alpha=0.7, label='Anomalies', zorder=3)\n",
    "    # Highlight persistent anomalies\n",
    "    persistent_dates = results['persistent_anomaly_flags'][results['persistent_anomaly_flags']].index\n",
    "    if len(persistent_dates) > 0:\n",
    "        axes[3].scatter(persistent_dates, results['residual'].loc[persistent_dates], \n",
    "                       color='red', s=25, alpha=0.8, label='Persistent Anomalies', \n",
    "                       marker='x', zorder=4)\n",
    "    axes[3].axhline(0, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    axes[3].set_ylabel('Residual', fontsize=11)\n",
    "    axes[3].legend(loc='best', fontsize=9)\n",
    "    axes[3].grid(alpha=0.3)\n",
    "    \n",
    "    # Robust Z-scores\n",
    "    axes[4].plot(results['residual_z_scores'].index, results['residual_z_scores'].values, \n",
    "                 linewidth=0.6, color='steelblue', alpha=0.6)\n",
    "    axes[4].axhline(-3, color='red', linestyle='--', linewidth=1.5, alpha=0.7, \n",
    "                    label='Anomaly Threshold')\n",
    "    axes[4].axhline(0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "    axes[4].fill_between(results['residual_z_scores'].index, -3, \n",
    "                         results['residual_z_scores'].values.min(), \n",
    "                         where=(results['residual_z_scores'].values < -3),\n",
    "                         alpha=0.2, color='red', label='Anomaly Region')\n",
    "    axes[4].set_ylabel('Robust Z-score\\n(MAD-based)', fontsize=11)\n",
    "    axes[4].set_xlabel('Date', fontsize=11)\n",
    "    axes[4].legend(loc='best', fontsize=9)\n",
    "    axes[4].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    from src.utils import sanitize_filename\n",
    "    stl_dir = PLOTS_DIR / \"stl_analysis\"\n",
    "    stl_dir.mkdir(parents=True, exist_ok=True)\n",
    "    safe_name = sanitize_filename(test_park)\n",
    "    plot_path = stl_dir / f\"stl_decomposition_{safe_name}.png\"\n",
    "    fig.savefig(plot_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"\\nâœ“ Plot saved: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Create Markdown Report\n",
    "    from datetime import datetime\n",
    "    report_date = pd.Timestamp.now()\n",
    "    report_filename = f\"report_weekly_stl_{report_date.strftime('%Y-%m-%d')}.md\"\n",
    "    report_path = WORKSPACE_ROOT / \"docs\" / report_filename\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        # Header\n",
    "        f.write(f\"# STL Decomposition & Degradation Analysis Report\\n\")\n",
    "        f.write(f\"**Report Date:** {report_date.strftime('%B %d, %Y')}\\n\\n\")\n",
    "        f.write(f\"**Park:** {test_park}\\n\")\n",
    "        f.write(f\"**Park Label:** {short_label(test_park)}\\n\")\n",
    "        f.write(f\"**Capacity:** {parse_kwp_from_header(test_park):.0f} kWp\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        f.write(\"## Executive Summary\\n\\n\")\n",
    "        f.write(\"This report presents a comprehensive time series decomposition and degradation analysis \")\n",
    "        f.write(f\"of {short_label(test_park)} using Seasonal-Trend decomposition using LOESS (STL) \")\n",
    "        f.write(\"with robust anomaly detection based on MAD (Median Absolute Deviation) statistics.\\n\\n\")\n",
    "        \n",
    "        # Key Findings\n",
    "        f.write(\"## Key Findings\\n\\n\")\n",
    "        f.write(f\"### Data Overview\\n\")\n",
    "        f.write(f\"- **Data Range:** {results['date_range']}\\n\")\n",
    "        f.write(f\"- **Total Data Points:** {results['data_points']:,} days\\n\")\n",
    "        f.write(f\"- **Analysis Period:** {(results['data_points'] / 365.25):.1f} years\\n\\n\")\n",
    "        \n",
    "        f.write(f\"### Degradation Analysis\\n\")\n",
    "        degradation_sign = \"ðŸ“‰\" if results['degradation_rate'] < 0 else \"ðŸ“ˆ\"\n",
    "        f.write(f\"- **Monthly Degradation Rate:** {results['degradation_rate']:+.4f}% per month {degradation_sign}\\n\")\n",
    "        f.write(f\"- **Annual Degradation Rate:** {results['annual_degradation']:+.4f}% per year\\n\")\n",
    "        f.write(f\"- **Trend RÂ² (Goodness of Fit):** {results['trend_r_squared']:.4f}\\n\")\n",
    "        f.write(f\"- **Trend Slope:** {results['trend_slope']:.6e} kWh/day\\n\\n\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if abs(results['annual_degradation']) > 1.0:\n",
    "            severity = \"âš ï¸ **High degradation** - Requires investigation\"\n",
    "        elif abs(results['annual_degradation']) > 0.5:\n",
    "            severity = \"âš ï¸ **Moderate degradation** - Monitor closely\"\n",
    "        else:\n",
    "            severity = \"âœ… **Normal degradation** - Within expected range\"\n",
    "        f.write(f\"**Interpretation:** {severity}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"### Anomaly Detection\\n\")\n",
    "        f.write(f\"- **Total Anomalies Detected:** {results['anomaly_count']} days ({results['anomaly_percentage']:.2f}%)\\n\")\n",
    "        f.write(f\"- **Persistent Anomalies:** {results['persistent_anomaly_count']} days ({results['persistent_anomaly_percentage']:.2f}%)\\n\")\n",
    "        f.write(f\"- **Anomaly Threshold:** Z-score < -3.0\\n\")\n",
    "        f.write(f\"- **Persistence Criterion:** â‰¥ 2 consecutive days\\n\")\n",
    "        f.write(f\"- **Residual MAD:** {results['mean_residual_mad']:.2f} kWh\\n\\n\")\n",
    "        \n",
    "        # Plot\n",
    "        f.write(\"## STL Decomposition Visualization\\n\\n\")\n",
    "        rel_path = Path(\"..\") / \"plots\" / \"stl_analysis\" / plot_path.name\n",
    "        f.write(f\"![STL Decomposition - {short_label(test_park)}]({rel_path})\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Plot Components\\n\\n\")\n",
    "        f.write(\"1. **Observed:** Original time series data showing daily energy generation\\n\")\n",
    "        f.write(\"2. **Trend:** Long-term trend component with linear regression fit (red dashed line)\\n\")\n",
    "        f.write(\"3. **Seasonal:** Periodic seasonal pattern (365-day period)\\n\")\n",
    "        f.write(\"4. **Residual:** Remaining variation after removing trend and seasonality\\n\")\n",
    "        f.write(\"   - ðŸŸ  Orange dots: Individual anomalies (Z-score < -3)\\n\")\n",
    "        f.write(\"   - ðŸ”´ Red X markers: Persistent anomalies (â‰¥2 consecutive days)\\n\")\n",
    "        f.write(\"5. **Robust Z-scores:** MAD-based standardized residuals with anomaly threshold\\n\\n\")\n",
    "        \n",
    "        # Methodology\n",
    "        f.write(\"## Methodology\\n\\n\")\n",
    "        f.write(\"### STL Decomposition\\n\")\n",
    "        f.write(\"- **Method:** Seasonal-Trend decomposition using LOESS\\n\")\n",
    "        f.write(\"- **Period:** 365 days (annual seasonality)\\n\")\n",
    "        f.write(\"- **Robust Fitting:** Enabled (resistant to outliers)\\n\")\n",
    "        f.write(\"- **Log Transformation:** Not applied\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Anomaly Detection\\n\")\n",
    "        f.write(\"- **Robust Z-Score Calculation:**\\n\")\n",
    "        f.write(\"  - Based on Median Absolute Deviation (MAD)\\n\")\n",
    "        f.write(\"  - Formula: `Z = (residual - median) / (1.4826 Ã— MAD)`\\n\")\n",
    "        f.write(\"  - Threshold: Z < -3.0 (â‰ˆ99.7% confidence for normal distribution)\\n\")\n",
    "        f.write(\"- **Persistence Filter:**\\n\")\n",
    "        f.write(\"  - Flags clusters of consecutive anomalous days\\n\")\n",
    "        f.write(\"  - Helps distinguish systematic issues from random fluctuations\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Degradation Calculation\\n\")\n",
    "        f.write(\"- **Linear Regression:** Fitted to trend component\\n\")\n",
    "        f.write(\"- **Monthly Medians:** Aggregated from daily trend values\\n\")\n",
    "        f.write(\"- **Rate Computation:** Relative change per unit time\\n\")\n",
    "        f.write(\"  - Daily slope converted to monthly/annual percentages\\n\")\n",
    "        f.write(\"  - Normalized by mean trend value\\n\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(\"## Recommendations\\n\\n\")\n",
    "        \n",
    "        if results['persistent_anomaly_count'] > 0:\n",
    "            f.write(\"### Anomaly Investigation\\n\")\n",
    "            f.write(\"1. **Review Persistent Anomalies:** Investigate the causes of multi-day performance drops\\n\")\n",
    "            f.write(\"2. **Correlate with Maintenance Records:** Check if anomalies align with maintenance events\\n\")\n",
    "            f.write(\"3. **Weather Correlation:** Verify if anomalies coincide with extreme weather events\\n\")\n",
    "            f.write(\"4. **Equipment Inspection:** Consider on-site inspection for persistent issues\\n\\n\")\n",
    "        \n",
    "        if abs(results['annual_degradation']) > 0.5:\n",
    "            f.write(\"### Degradation Management\\n\")\n",
    "            f.write(\"1. **Monitor Trend:** Track degradation rate over time to detect acceleration\\n\")\n",
    "            f.write(\"2. **Compare with Specifications:** Verify if degradation is within warranty limits\\n\")\n",
    "            f.write(\"3. **Predictive Maintenance:** Plan interventions based on degradation trajectory\\n\")\n",
    "            f.write(\"4. **Financial Impact:** Update revenue projections to account for degradation\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Data Quality\\n\")\n",
    "        f.write(\"1. **Fill Data Gaps:** Address any missing data periods to improve analysis\\n\")\n",
    "        f.write(\"2. **Sensor Calibration:** Verify measurement accuracy, especially if anomalies are frequent\\n\")\n",
    "        f.write(\"3. **Regular Monitoring:** Repeat this analysis quarterly to track changes\\n\\n\")\n",
    "        \n",
    "        # Technical Details\n",
    "        f.write(\"## Technical Details\\n\\n\")\n",
    "        f.write(\"### Model Parameters\\n\")\n",
    "        f.write(f\"- Seasonal Period: {results['parameters']['period']} days\\n\")\n",
    "        f.write(f\"- Robust Fitting: {'Enabled' if results['parameters']['robust'] else 'Disabled'}\\n\")\n",
    "        f.write(f\"- Anomaly Threshold: {results['parameters']['anomaly_threshold']} (Z-score)\\n\")\n",
    "        f.write(f\"- Minimum Consecutive Days: {results['parameters']['min_consecutive_days']}\\n\")\n",
    "        f.write(f\"- Log Transformation: {'Applied' if results['parameters']['apply_log'] else 'Not applied'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Statistical Metrics\\n\")\n",
    "        f.write(f\"- **Trend Slope:** {results['trend_slope']:.6e} kWh/day\\n\")\n",
    "        f.write(f\"- **Trend Intercept:** {results['trend_intercept']:.2f} kWh\\n\")\n",
    "        f.write(f\"- **RÂ² (Trend Fit):** {results['trend_r_squared']:.4f}\\n\")\n",
    "        f.write(f\"- **Median Residual:** 0 (by definition)\\n\")\n",
    "        f.write(f\"- **MAD (Residuals):** {results['mean_residual_mad']:.2f} kWh\\n\\n\")\n",
    "        \n",
    "        # Footer\n",
    "        f.write(\"---\\n\\n\")\n",
    "        f.write(\"### References\\n\\n\")\n",
    "        f.write(\"- Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). \")\n",
    "        f.write(\"STL: A seasonal-trend decomposition procedure based on loess. *Journal of Official Statistics*, 6(1), 3-73.\\n\")\n",
    "        f.write(\"- Leys, C., Ley, C., Klein, O., Bernard, P., & Licata, L. (2013). \")\n",
    "\n",
    "        f.write(\"Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median. \")    \n",
    "        print(\"Analysis complete!\")\n",
    "\n",
    "        f.write(\"*Journal of Experimental Social Psychology*, 49(4), 764-766.\\n\\n\")    \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "        print(f\"âœ“ Report saved: {report_path}\")\n",
    "\n",
    "        f.write(f\"*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac985c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weekly_technical_report_for_all_parks(\n",
    "    daily_df,\n",
    "    pi_df,\n",
    "    daily_historical_df,\n",
    "    report_date=None,\n",
    "    price_per_kwh=0.2,\n",
    "    currency=\"EUR\",\n",
    "    save_dir=None,\n",
    "    version=\"v1\",\n",
    "    dpi=150,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the weekly technical department report with key operational charts and insights.\n",
    "\n",
    "    Outputs markdown: docs/report_tech_weekly_<YYYYMMDD>_<version>.md\n",
    "    Saves plots in: plots/weekly_analysis/\n",
    "    Plot naming: figN_description_<YYYYMMDD>_<version>.png\n",
    "    \n",
    "    Includes plots:\n",
    "      - fig1: Daily energy time series grid (all parks)\n",
    "      - fig2: PI heatmap (full period)\n",
    "      - fig3: PI heatmap (month-to-date)\n",
    "      - fig4: Month-to-date total REVENUE for all parks by year (bar chart)\n",
    "      - fig5: Month-to-date REVENUE per park by year (grid)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Defaults\n",
    "    if save_dir is None:\n",
    "        save_dir = PLOTS_DIR\n",
    "    save_dir = Path(save_dir)\n",
    "    \n",
    "    # Create weekly_analysis subdirectory\n",
    "    weekly_dir = save_dir / \"weekly_analysis\"\n",
    "    weekly_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    if report_date is None:\n",
    "        report_date = pd.Timestamp.now()\n",
    "    else:\n",
    "        report_date = pd.Timestamp(report_date)\n",
    "\n",
    "    # Date string for filenames (YYYYMMDD format)\n",
    "    date_str = report_date.strftime('%Y%m%d')\n",
    "    \n",
    "    month_start = pd.Timestamp(year=report_date.year, month=report_date.month, day=1)\n",
    "    month_name = month_start.strftime('%B %Y')\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"GENERATING WEEKLY TECHNICAL REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Report Date: {report_date.strftime('%B %d, %Y')}\")\n",
    "    print(f\"Version: {version}\")\n",
    "    print(f\"Save Directory: {weekly_dir}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1) Daily energy time series grid\n",
    "    print(\"1) Daily energy time series grid ...\")\n",
    "    ts_path = lineplot_timeseries_per_column(\n",
    "        daily_df,\n",
    "        title_prefix=\"Daily Energy\",\n",
    "        ylabel=\"Energy [kWh]\",\n",
    "        ncols=3,\n",
    "        sharex=True,\n",
    "        sharey=False,\n",
    "        save=True,\n",
    "        save_dir=weekly_dir,\n",
    "        base_filename=f\"fig1_daily_energy_timeseries_{date_str}_{version}\",\n",
    "        dpi=dpi,\n",
    "        fmt=\"png\",\n",
    "    )\n",
    "\n",
    "    # 2) PI heatmap - full period\n",
    "    print(\"2) PI heatmap - full period ...\")\n",
    "    pi_full_path = plot_heatmap(\n",
    "        pi_df,\n",
    "        \"PI_PVGIS = Measured / PVGIS expected\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.5,\n",
    "        save=True,\n",
    "        save_dir=weekly_dir,\n",
    "        base_filename=f\"fig2_pi_heatmap_full_{date_str}_{version}\",\n",
    "        dpi=dpi,\n",
    "        fmt=\"png\",\n",
    "    )\n",
    "\n",
    "    # 3) PI heatmap - month to date\n",
    "    print(\"3) PI heatmap - month-to-date ...\")\n",
    "    pi_mtd_path = plot_heatmap(\n",
    "        pi_df,\n",
    "        f\"PI_PVGIS - {month_start.strftime('%B %Y')}\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.5,\n",
    "        start_date=month_start,\n",
    "        end_date=report_date,\n",
    "        save=True,\n",
    "        save_dir=weekly_dir,\n",
    "        base_filename=f\"fig3_pi_heatmap_mtd_{date_str}_{version}\",\n",
    "        dpi=dpi,\n",
    "        fmt=\"png\",\n",
    "    )\n",
    "\n",
    "    # 4) Month-to-date total REVENUE for all parks by year (using plot_revenue_by_year)\n",
    "    print(\"4) Month-to-date total REVENUE for all parks by year ...\")\n",
    "    # Compute MTD energy per year for all parks combined\n",
    "    mtd_total_energy = analyze_month_to_date_by_year(\n",
    "        daily_historical_df,\n",
    "        column=None,\n",
    "        aggregation='sum',\n",
    "        current_date=report_date,\n",
    "    )\n",
    "    \n",
    "    # Convert energy to revenue\n",
    "    mtd_total_revenue = mtd_total_energy * price_per_kwh\n",
    "    \n",
    "    # Use the existing plot_revenue_by_year function\n",
    "    fig_revenue, revenue_by_year_path = plot_revenue_by_year(\n",
    "        mtd_total_revenue,\n",
    "        title=f\"Month-to-Date REVENUE by Year â€” All Parks ({month_start.strftime('%B')} 1-{report_date.day})\",\n",
    "        price_per_kwh=0.2,\n",
    "        currency=\"EUR\",\n",
    "        save=True,\n",
    "        save_dir=weekly_dir,\n",
    "        base_filename=f\"fig4_revenue_mtd_all_parks_{date_str}_{version}\",\n",
    "        dpi=dpi,\n",
    "        fmt=\"png\"\n",
    "    )\n",
    "\n",
    "    # 5) Month-to-date REVENUE per-park grid by year\n",
    "    print(\"5) Month-to-date REVENUE per-park grid by year ...\")\n",
    "    mtd_grid_path = plot_mtd_revenue_by_year_grid(\n",
    "        daily_historical_df=daily_historical_df,\n",
    "        current_date=report_date,\n",
    "        price_per_kwh=price_per_kwh,\n",
    "        currency=currency,\n",
    "        ncols=3,\n",
    "        save=True,\n",
    "        save_dir=weekly_dir,\n",
    "        base_filename=f\"fig5_revenue_mtd_grid_{date_str}_{version}\",\n",
    "        dpi=dpi,\n",
    "        fmt=\"png\",\n",
    "    )\n",
    "\n",
    "    # Compose Markdown report\n",
    "    print(\"\\nWriting markdown report ...\")\n",
    "    report_filename = f\"report_tech_weekly_{date_str}_{version}.md\"\n",
    "    report_path = WORKSPACE_ROOT / \"docs\" / report_filename\n",
    "\n",
    "    # Relative paths for images (docs/ -> plots/weekly_analysis/)\n",
    "    rel_ts_path = Path(\"../plots/weekly_analysis\") / Path(ts_path).name if ts_path else None\n",
    "    rel_pi_full = Path(\"../plots/weekly_analysis\") / Path(pi_full_path).name if pi_full_path else None\n",
    "    rel_pi_mtd = Path(\"../plots/weekly_analysis\") / Path(pi_mtd_path).name if pi_mtd_path else None\n",
    "    rel_revenue_by_year = Path(\"../plots/weekly_analysis\") / Path(revenue_by_year_path).name if revenue_by_year_path else None\n",
    "    rel_mtd_grid = Path(\"../plots/weekly_analysis\") / Path(mtd_grid_path).name if mtd_grid_path else None\n",
    "\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        # Header\n",
    "        f.write(\"# Weekly PV Technical Report\\n\")\n",
    "        f.write(f\"**Report Date:** {report_date.strftime('%B %d, %Y')}\\n\")\n",
    "        f.write(f\"**Version:** {version}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        # Executive Summary\n",
    "        f.write(\"## Executive Summary\\n\\n\")\n",
    "        f.write(\"This weekly technical report summarizes operational performance across all PV parks, \"\n",
    "                \"with an emphasis on time-series behavior, month-to-date performance indicators (PI), \"\n",
    "                \"and revenue totals.\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        # Daily Energy Grid\n",
    "        f.write(\"## 1. Daily Energy Time Series (All Parks)\\n\\n\")\n",
    "        if rel_ts_path:\n",
    "            f.write(f\"![Daily Energy Time Series]({rel_ts_path})\\n\\n\")\n",
    "        f.write(\"**Notes:**\\n\")\n",
    "        f.write(\"- Check for gaps or flatlines indicating telemetry or data acquisition issues.\\n\")\n",
    "        f.write(\"- Compare inter-park variability to expected seasonal patterns.\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        # PI Heatmap Full Period\n",
    "        f.write(\"## 2. Performance Index (PI) Heatmap â€” Full Period\\n\\n\")\n",
    "        if rel_pi_full:\n",
    "            f.write(f\"![PI Heatmap - Full Period]({rel_pi_full})\\n\\n\")\n",
    "        f.write(\"**Interpretation:** PI â‰ˆ 1 implies measured energy aligns with PVGIS expectation. \"\n",
    "                \"Sustained PI < 0.8 suggests underperformance; PI > 1.2 may indicate measurement anomalies.\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        # PI Heatmap Month-to-Date\n",
    "        f.write(f\"## 3. PI Heatmap â€” {month_start.strftime('%B %Y')} (Month-to-Date)\\n\\n\")\n",
    "        if rel_pi_mtd:\n",
    "            f.write(f\"![PI Heatmap - MTD]({rel_pi_mtd})\\n\\n\")\n",
    "        f.write(\"**Operational Checkpoints:**\\n\")\n",
    "        f.write(\"- Investigate parks with sustained blue regions (PI < 0.8).\\n\")\n",
    "        f.write(\"- Correlate anomalies with weather, outages, and maintenance logs.\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        # MTD revenue totals (all parks combined)\n",
    "        f.write(\"## 4. Month-to-Date Revenue by Year â€” All Parks\\n\\n\")\n",
    "        f.write(f\"Period: {month_start.strftime('%B')} 1â€“{report_date.day}\\n\\n\")\n",
    "        if rel_revenue_by_year:\n",
    "            f.write(f\"![Revenue MTD - All Parks]({rel_revenue_by_year})\\n\\n\")\n",
    "        f.write(\"**Insights:**\\n\")\n",
    "        best_year = mtd_total_revenue.idxmax()\n",
    "        worst_year = mtd_total_revenue.idxmin()\n",
    "        f.write(f\"- Highest MTD revenue: {best_year} ({mtd_total_revenue[best_year]:,.2f} {currency})\\n\")\n",
    "        f.write(f\"- Lowest MTD revenue: {worst_year} ({mtd_total_revenue[worst_year]:,.2f} {currency})\\n\")\n",
    "        if len(mtd_total_revenue) > 1:\n",
    "            growth = ((mtd_total_revenue.iloc[-1] - mtd_total_revenue.iloc[0]) / max(mtd_total_revenue.iloc[0], 1e-6)) * 100\n",
    "            f.write(f\"- Overall change: {growth:+.2f}% from {mtd_total_revenue.index[0]} to {mtd_total_revenue.index[-1]}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        # MTD per-park grid (revenue per kWp)\n",
    "        f.write(\"## 5. Month-to-Date Revenue per Park by Year (Grid)\\n\\n\")\n",
    "        if rel_mtd_grid:\n",
    "            f.write(f\"![MTD Revenue Grid]({rel_mtd_grid})\\n\\n\")\n",
    "        f.write(\"**Usage:** Highlights per-park month-to-date revenue against historical years. \"\n",
    "                \"Use alongside PI heatmaps to differentiate revenue-impacting underperformance from data issues.\\n\\n\")\n",
    "\n",
    "        # Footer\n",
    "        f.write(\"---\\n\\n\")\n",
    "        f.write(f\"*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\")\n",
    "\n",
    "    print(f\"\\nâœ“ Technical report saved: {report_path}\")\n",
    "    return report_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19afb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the weekly technical report\n",
    "print(\"\\nðŸ”§ Creating weekly technical report (technical department)...\\n\")\n",
    "weekly_tech_report = create_weekly_technical_report_for_all_parks(\n",
    "    daily_df=daily,\n",
    "    pi_df=pi,\n",
    "    daily_historical_df=daily_historical,\n",
    "    report_date=None,  # Uses today's date\n",
    "    price_per_kwh=0.2,\n",
    "    currency=\"EUR\",\n",
    "    save_dir=PLOTS_DIR,\n",
    "    dpi=180,\n",
    ")\n",
    "print(f\"\\nðŸ“„ Technical Report generated: {weekly_tech_report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pv-kpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
